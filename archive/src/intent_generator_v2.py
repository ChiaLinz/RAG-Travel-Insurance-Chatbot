"""
Intent Generator V2 - å„ªåŒ–ç‰ˆæ„åœ–ç”Ÿæˆå™¨

ä¸»è¦å„ªåŒ–ï¼š
1. é‡å°æ€§å•æ³•ç”Ÿæˆï¼ˆå€åˆ†ã€Œå»¶èª¤ã€vsã€Œéºå¤±ã€vsã€Œæå¤±ã€ï¼‰
2. å°æ¯”å¼æ„åœ–ï¼ˆå¹«åŠ©ç”¨æˆ¶å€åˆ†ç›¸ä¼¼æ¦‚å¿µï¼‰
3. å¤šæ¨£åŒ–å•æ³•ï¼ˆåŒç¾©è©è®Šé«”ã€å£èªåŒ–è¡¨é”ï¼‰
4. è² å‘æ„åœ–ï¼ˆæ˜ç¢ºä»€éº¼ä¸èƒ½åšï¼‰
5. æ¢ä»¶çµ„åˆæ„åœ–ï¼ˆè¤‡é›œå ´æ™¯ï¼‰
"""

import json
import os
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
from openai import OpenAI
from dotenv import load_dotenv
from config import INDEX_DIR
import time


# ==================== åˆå§‹åŒ– ====================

load_dotenv()
client = OpenAI()


# ==================== æ•¸æ“šçµæ§‹ ====================

@dataclass
class Intent:
    """æ„åœ–çµæ§‹"""
    intent_id: str
    clause_id: str
    item_no: Optional[str]
    subitem_no: Optional[str]
    
    # æ ¸å¿ƒå…§å®¹
    user_query: str
    excerpt: str
    
    # è©³ç´°ä¿¡æ¯
    conditions: List[str]
    exceptions: List[str]
    referenced_clauses: List[str]
    
    # åˆ†é¡
    category: str
    
    # V2 æ–°å¢å­—æ®µ
    query_type: str = "ç›´æ¥æŸ¥è©¢"  # ç›´æ¥æŸ¥è©¢ã€å°æ¯”æŸ¥è©¢ã€æ¢ä»¶æŸ¥è©¢ã€è² å‘æŸ¥è©¢
    semantic_tags: List[str] = None  # èªç¾©æ¨™ç±¤ï¼ˆå»¶èª¤ã€éºå¤±ã€æå¤±ç­‰ï¼‰
    difficulty: str = "ç°¡å–®"  # ç°¡å–®ã€ä¸­ç­‰ã€è¤‡é›œ
    
    def __post_init__(self):
        if self.semantic_tags is None:
            self.semantic_tags = []
    
    def to_dict(self):
        return asdict(self)


# ==================== LLM æç¤ºè©ï¼ˆV2 å¢å¼·ç‰ˆï¼‰====================

INTENT_GENERATION_PROMPT_V2 = """ä½ æ˜¯ä¿éšªæ¢æ¬¾åˆ†æå°ˆå®¶ã€‚è«‹ä»”ç´°åˆ†æä»¥ä¸‹æ¢æ–‡ï¼Œç”Ÿæˆ 5-8 å€‹ä¸åŒé¡å‹çš„ç”¨æˆ¶å¯èƒ½å•é¡Œï¼ˆæ„åœ–ï¼‰ã€‚

æ¢æ–‡ä¿¡æ¯ï¼š
ç« ç¯€ï¼š{chapter_info}
æ¢æ–‡ç·¨è™Ÿï¼š{clause_no}
æ¢æ–‡æ¨™é¡Œï¼š{clause_title}
æ¢æ–‡åŠŸèƒ½ï¼š{clause_function}
ä¿éšªé¡å‹ï¼š{insurance_types}
å‹•ä½œé—œéµè©ï¼š{action_keywords}
æ¢æ–‡å…§å®¹ï¼š
{context}

**é‡è¦æç¤º**ï¼š
1. å¿…é ˆç”Ÿæˆå¤šæ¨£åŒ–çš„å•æ³•ï¼ŒåŒ…æ‹¬ï¼š
   - ç›´æ¥æŸ¥è©¢ï¼ˆæœ€å¸¸è¦‹çš„å•æ³•ï¼‰
   - å£èªåŒ–è¡¨é”ï¼ˆç”¨æˆ¶å¯¦éš›æœƒæ€éº¼å•ï¼‰
   - ç‰¹å®šå ´æ™¯ï¼ˆå…·é«”æƒ…å¢ƒä¸‹çš„å•é¡Œï¼‰
   - è² å‘æŸ¥è©¢ï¼ˆä»€éº¼æƒ…æ³ä¸‹ä¸èƒ½/ä¸æœƒï¼‰

2. ç‰¹åˆ¥æ³¨æ„å€åˆ†ç›¸ä¼¼æ¦‚å¿µï¼š
   - "å»¶èª¤" vs "éºå¤±" vs "æå¤±" vs "å–æ¶ˆ" vs "æ›´æ”¹"
   - æ¯å€‹è©çš„å•æ³•éƒ½è¦æ˜ç¢ºå€åˆ†

3. å¦‚æœæ˜¯ã€Œä¸ä¿äº‹é …ã€æ¢æ–‡ï¼Œå¿…é ˆç”Ÿæˆè² å‘æ„åœ–ï¼š
   - "å“ªäº›æƒ…æ³ä¸ç†è³ ï¼Ÿ"
   - "ä»€éº¼æ™‚å€™ä¸èƒ½ç”³è«‹ï¼Ÿ"

è«‹ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
  "intents": [
    {{
      "user_query": "ç”¨æˆ¶å¯èƒ½çš„å•é¡Œï¼ˆè¦è‡ªç„¶ã€å£èªåŒ–ï¼‰",
      "excerpt": "å›ç­”è©²å•é¡Œçš„é—œéµæ¢æ–‡æ‘˜éŒ„ï¼ˆä¸è¶…é100å­—ï¼‰",
      "conditions": ["é©ç”¨æ¢ä»¶1", "é©ç”¨æ¢ä»¶2"],
      "exceptions": ["ä¾‹å¤–æƒ…æ³1"],
      "referenced_clauses": ["å¼•ç”¨çš„å…¶ä»–æ¢æ–‡"],
      "category": "åˆ†é¡ï¼ˆè³ å„Ÿç¯„åœ/ç†è³ æ¢ä»¶/é™¤å¤–è²¬ä»»/ç”³è«‹æµç¨‹/å®šç¾©èªªæ˜ï¼‰",
      "query_type": "æŸ¥è©¢é¡å‹ï¼ˆç›´æ¥æŸ¥è©¢/å°æ¯”æŸ¥è©¢/æ¢ä»¶æŸ¥è©¢/è² å‘æŸ¥è©¢ï¼‰",
      "semantic_tags": ["èªç¾©æ¨™ç±¤ï¼Œå¦‚ï¼šå»¶èª¤ã€éºå¤±ã€ç«Šç›œç­‰"],
      "difficulty": "é›£åº¦ï¼ˆç°¡å–®/ä¸­ç­‰/è¤‡é›œï¼‰"
    }}
  ]
}}

**ç¯„ä¾‹**ï¼ˆå‡è¨­æ˜¯ã€Œè¡Œææå¤±ä¿éšªæ‰¿ä¿ç¯„åœã€æ¢æ–‡ï¼‰ï¼š
{{
  "intents": [
    {{
      "user_query": "è¡Œæè¢«å·äº†å¯ä»¥ç†è³ å—ï¼Ÿ",
      "excerpt": "å› ç«Šç›œã€å¼·ç›œèˆ‡æ¶å¥ªå°è‡´è¡Œæéºå¤±å¯ä»¥ç†è³ ",
      "conditions": ["ç«Šç›œã€å¼·ç›œæˆ–æ¶å¥ª", "ç½®æ–¼è¡Œæç®±å…§", "æµ·å¤–æ—…è¡ŒæœŸé–“"],
      "exceptions": [],
      "referenced_clauses": [],
      "category": "è³ å„Ÿç¯„åœ",
      "query_type": "ç›´æ¥æŸ¥è©¢",
      "semantic_tags": ["éºå¤±", "ç«Šç›œ"],
      "difficulty": "ç°¡å–®"
    }},
    {{
      "user_query": "è¡Œæè¢«èˆªç©ºå…¬å¸å¼„ä¸Ÿäº†æ€éº¼è¾¦ï¼Ÿ",
      "excerpt": "è¨—é‹è¡Œæå› æ¥­è€…è™•ç†å¤±ç•¶å°è‡´éºå¤±å¯ç†è³ ",
      "conditions": ["è¨—é‹è¡Œæ", "é ˜æœ‰è¨—é‹å–®", "æ¥­è€…è™•ç†å¤±ç•¶"],
      "exceptions": [],
      "referenced_clauses": [],
      "category": "è³ å„Ÿç¯„åœ",
      "query_type": "ç›´æ¥æŸ¥è©¢",
      "semantic_tags": ["éºå¤±", "è¨—é‹"],
      "difficulty": "ç°¡å–®"
    }},
    {{
      "user_query": "è¡Œæå»¶èª¤å’Œè¡Œæéºå¤±æœ‰ä»€éº¼ä¸åŒï¼Ÿ",
      "excerpt": "è¡Œæéºå¤±æ˜¯æŒ‡æ¯€æã€æ»…å¤±ï¼›å»¶èª¤æ˜¯æŒ‡æœªèƒ½åŠæ™‚é ˜å–",
      "conditions": [],
      "exceptions": [],
      "referenced_clauses": ["ç¬¬ä¸‰åå…­æ¢"],
      "category": "å®šç¾©èªªæ˜",
      "query_type": "å°æ¯”æŸ¥è©¢",
      "semantic_tags": ["éºå¤±", "å»¶èª¤", "å°æ¯”"],
      "difficulty": "ä¸­ç­‰"
    }},
    {{
      "user_query": "å“ªäº›æ±è¥¿ä¸Ÿäº†ä¸èƒ½ç†è³ ï¼Ÿ",
      "excerpt": "å•†æ¥­ç”¨å“ã€è²¨å¹£ã€è­‰åˆ¸ç­‰ä¸åœ¨ç†è³ ç¯„åœ",
      "conditions": [],
      "exceptions": ["å•†æ¥­ç”¨å“", "è²¨å¹£", "è­‰åˆ¸"],
      "referenced_clauses": ["ç¬¬å››åæ¢"],
      "category": "é™¤å¤–è²¬ä»»",
      "query_type": "è² å‘æŸ¥è©¢",
      "semantic_tags": ["éºå¤±", "ä¸ä¿"],
      "difficulty": "ç°¡å–®"
    }}
  ]
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–èªªæ˜ï¼š"""


ITEM_INTENT_GENERATION_PROMPT_V2 = """ä½ æ˜¯ä¿éšªæ¢æ¬¾åˆ†æå°ˆå®¶ã€‚è«‹åˆ†æä»¥ä¸‹é …ç›®å…§å®¹ï¼Œç”Ÿæˆ 2-3 å€‹ç”¨æˆ¶å¯èƒ½å•é¡Œã€‚

æ¯æ¢æ–‡ï¼š{clause_no} {clause_title}
æ¢æ–‡åŠŸèƒ½ï¼š{clause_function}
é …ç›®ç·¨è™Ÿï¼š{item_no}
é …ç›®å…§å®¹ï¼š
{item_context}

å‹•ä½œé—œéµè©ï¼š{action_keywords}

**æ³¨æ„äº‹é …**ï¼š
1. å•æ³•è¦é‡å°é€™å€‹ç‰¹å®šé …ç›®
2. å€åˆ†ä¸åŒçš„å‹•ä½œè©ï¼ˆå»¶èª¤/éºå¤±/æå¤±/å–æ¶ˆï¼‰
3. å¦‚æœæ˜¯ä¸ä¿äº‹é …çš„é …ç›®ï¼Œè¦ç”Ÿæˆè² å‘å•æ³•

è«‹ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
  "intents": [
    {{
      "user_query": "é‡å°é€™å€‹é …ç›®çš„å…·é«”å•é¡Œ",
      "excerpt": "é—œéµå…§å®¹æ‘˜éŒ„",
      "conditions": ["æ¢ä»¶"],
      "exceptions": ["ä¾‹å¤–"],
      "referenced_clauses": ["å¼•ç”¨"],
      "category": "åˆ†é¡",
      "query_type": "æŸ¥è©¢é¡å‹",
      "semantic_tags": ["èªç¾©æ¨™ç±¤"],
      "difficulty": "é›£åº¦"
    }}
  ]
}}

åªè¿”å› JSONï¼š"""


# ==================== å°æ¯”æ„åœ–ç”Ÿæˆ ====================

COMPARISON_INTENT_TEMPLATE = {
    "è¡Œæå»¶èª¤_vs_è¡Œææå¤±": {
        "user_query": "è¡Œæå»¶èª¤å’Œè¡Œæéºå¤±æœ‰ä»€éº¼å·®åˆ¥ï¼Ÿä»€éº¼æ™‚å€™ç®—å»¶èª¤ï¼Œä»€éº¼æ™‚å€™ç®—éºå¤±ï¼Ÿ",
        "excerpt": "è¡Œæå»¶èª¤æ˜¯æŒ‡æŠµé”6å°æ™‚å¾Œä»æœªé ˜å¾—ï¼›è¡Œææå¤±æ˜¯æŒ‡æ¯€æã€æ»…å¤±æˆ–éºå¤±",
        "category": "å®šç¾©èªªæ˜",
        "query_type": "å°æ¯”æŸ¥è©¢",
        "semantic_tags": ["å»¶èª¤", "éºå¤±", "å°æ¯”"],
        "difficulty": "ä¸­ç­‰",
        "related_clauses": ["ç¬¬ä¸‰åå…­æ¢", "ç¬¬ä¸‰åä¹æ¢"]
    },
    "ç­æ©Ÿå»¶èª¤_vs_æ—…ç¨‹å–æ¶ˆ": {
        "user_query": "ç­æ©Ÿå»¶èª¤å’Œæ—…ç¨‹å–æ¶ˆæœ‰ä»€éº¼ä¸åŒï¼Ÿåˆ†åˆ¥åœ¨ä»€éº¼æƒ…æ³ä¸‹ç†è³ ï¼Ÿ",
        "excerpt": "ç­æ©Ÿå»¶èª¤æ˜¯ç­æ©Ÿæ™šé»ï¼›æ—…ç¨‹å–æ¶ˆæ˜¯åœ¨å‡ºç™¼å‰å› ç‰¹å®šäº‹ç”±å–æ¶ˆæ•´å€‹è¡Œç¨‹",
        "category": "å®šç¾©èªªæ˜",
        "query_type": "å°æ¯”æŸ¥è©¢",
        "semantic_tags": ["å»¶èª¤", "å–æ¶ˆ", "å°æ¯”"],
        "difficulty": "ä¸­ç­‰",
        "related_clauses": ["ç¬¬äºŒåä¸ƒæ¢", "ç¬¬ä¸‰åæ¢"]
    },
    "æ—…ç¨‹å–æ¶ˆ_vs_æ—…ç¨‹æ›´æ”¹": {
        "user_query": "æ—…ç¨‹å–æ¶ˆå’Œæ—…ç¨‹æ›´æ”¹æœ‰ä½•å·®åˆ¥ï¼Ÿ",
        "excerpt": "æ—…ç¨‹å–æ¶ˆæ˜¯å‡ºç™¼å‰å…¨éƒ¨å–æ¶ˆï¼›æ—…ç¨‹æ›´æ”¹æ˜¯æ—…è¡Œä¸­å› æ•…è®Šæ›´è¡Œç¨‹",
        "category": "å®šç¾©èªªæ˜",
        "query_type": "å°æ¯”æŸ¥è©¢",
        "semantic_tags": ["å–æ¶ˆ", "æ›´æ”¹", "å°æ¯”"],
        "difficulty": "ä¸­ç­‰",
        "related_clauses": ["ç¬¬äºŒåä¸ƒæ¢", "ç¬¬ä¸‰åä¸‰æ¢"]
    },
    "ç«Šç›œ_vs_è™•ç†å¤±ç•¶": {
        "user_query": "è¡Œæè¢«å·å’Œè¢«èˆªç©ºå…¬å¸å¼„ä¸Ÿï¼Œç†è³ æœ‰ä»€éº¼ä¸åŒï¼Ÿ",
        "excerpt": "ç«Šç›œéœ€å ±è­¦ä¸¦å–å¾—å ±æ¡ˆè­‰æ˜ï¼›è™•ç†å¤±ç•¶éœ€æ¥­è€…å‡ºå…·äº‹æ•…è­‰æ˜",
        "category": "ç”³è«‹æµç¨‹",
        "query_type": "å°æ¯”æŸ¥è©¢",
        "semantic_tags": ["ç«Šç›œ", "éºå¤±", "å°æ¯”"],
        "difficulty": "ä¸­ç­‰",
        "related_clauses": ["ç¬¬ä¸‰åä¹æ¢", "ç¬¬å››åäºŒæ¢", "ç¬¬å››åä¸‰æ¢"]
    }
}


def generate_comparison_intents(chunks: List[Dict], intent_id_counter: List[int]) -> List[Intent]:
    """ç”Ÿæˆå°æ¯”æ„åœ–"""
    intents = []
    
    for comp_key, comp_data in COMPARISON_INTENT_TEMPLATE.items():
        intent_id = f"intent_{intent_id_counter[0]:04d}"
        intent_id_counter[0] += 1
        
        # æ‰¾åˆ°ç›¸é—œæ¢æ–‡
        related_clauses = comp_data.get("related_clauses", [])
        main_clause_id = None
        
        for chunk in chunks:
            clause_no = chunk["clause"]["clause_no"]
            if related_clauses and clause_no in related_clauses[0]:
                main_clause_id = chunk["clause"]["clause_id"]
                break
        
        if not main_clause_id:
            main_clause_id = chunks[0]["clause"]["clause_id"]  # é»˜èªç¬¬ä¸€æ¢
        
        intents.append(Intent(
            intent_id=intent_id,
            clause_id=main_clause_id,
            item_no=None,
            subitem_no=None,
            user_query=comp_data["user_query"],
            excerpt=comp_data["excerpt"],
            conditions=[],
            exceptions=[],
            referenced_clauses=related_clauses,
            category=comp_data["category"],
            query_type=comp_data["query_type"],
            semantic_tags=comp_data["semantic_tags"],
            difficulty=comp_data["difficulty"]
        ))
    
    print(f"âœ… å·²ç”Ÿæˆ {len(intents)} å€‹å°æ¯”æ„åœ–")
    return intents


# ==================== LLM èª¿ç”¨ ====================

def call_llm_for_intents(prompt: str, max_retries: int = 3) -> List[Dict]:
    """èª¿ç”¨ LLM ç”Ÿæˆæ„åœ–"""
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯å°ˆæ¥­çš„ä¿éšªæ¢æ¬¾åˆ†æå°ˆå®¶ï¼Œæ“…é•·å¾æ¢æ–‡ä¸­æå–å¤šæ¨£åŒ–çš„ç”¨æˆ¶æ„åœ–ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.4,  # ç¨å¾®æé«˜å‰µé€ æ€§
                max_tokens=2500
            )
            
            content = response.choices[0].message.content.strip()
            
            # ç§»é™¤ markdown
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            result = json.loads(content)
            return result.get("intents", [])
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON è§£æéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                print(f"âŒ ç„¡æ³•è§£æ: {content[:200]}")
                return []
            time.sleep(1)
            
        except Exception as e:
            print(f"âš ï¸  LLM éŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                return []
            time.sleep(2)
    
    return []


# ==================== æ„åœ–ç”Ÿæˆ ====================

def generate_clause_intents(chunk: Dict, intent_id_counter: List[int]) -> List[Intent]:
    """ç‚ºæ¢æ–‡ç”Ÿæˆæ„åœ–ï¼ˆV2 å¢å¼·ç‰ˆï¼‰"""
    clause = chunk["clause"]
    chapter_info = f"{chunk.get('chapter_no', '')} {chunk.get('chapter_title', '')}" if chunk.get('chapter_no') else "ç„¡ç« ç¯€"
    
    # æ§‹å»ºå¢å¼·çš„æç¤ºè©
    prompt = INTENT_GENERATION_PROMPT_V2.format(
        chapter_info=chapter_info,
        clause_no=clause["clause_no"],
        clause_title=clause["clause_title"],
        clause_function=clause.get("clause_function", "ä¸€èˆ¬è¦å®š"),
        insurance_types=", ".join(clause.get("insurance_types", ["å…¶ä»–"])),
        action_keywords=", ".join(clause.get("action_keywords", [])),
        context=clause["context"]
    )
    
    llm_intents = call_llm_for_intents(prompt)
    
    # è½‰æ›ç‚º Intent å°è±¡
    intents = []
    for llm_intent in llm_intents:
        intent_id = f"intent_{intent_id_counter[0]:04d}"
        intent_id_counter[0] += 1
        
        intents.append(Intent(
            intent_id=intent_id,
            clause_id=clause["clause_id"],
            item_no=None,
            subitem_no=None,
            user_query=llm_intent.get("user_query", ""),
            excerpt=llm_intent.get("excerpt", ""),
            conditions=llm_intent.get("conditions", []),
            exceptions=llm_intent.get("exceptions", []),
            referenced_clauses=llm_intent.get("referenced_clauses", []),
            category=llm_intent.get("category", "å…¶ä»–"),
            query_type=llm_intent.get("query_type", "ç›´æ¥æŸ¥è©¢"),
            semantic_tags=llm_intent.get("semantic_tags", []),
            difficulty=llm_intent.get("difficulty", "ç°¡å–®")
        ))
    
    return intents


def generate_item_intents(chunk: Dict, item: Dict, intent_id_counter: List[int]) -> List[Intent]:
    """ç‚ºé …ç›®ç”Ÿæˆæ„åœ–ï¼ˆV2 å¢å¼·ç‰ˆï¼‰"""
    clause = chunk["clause"]
    
    prompt = ITEM_INTENT_GENERATION_PROMPT_V2.format(
        clause_no=clause["clause_no"],
        clause_title=clause["clause_title"],
        clause_function=clause.get("clause_function", "ä¸€èˆ¬è¦å®š"),
        item_no=item["item_no"],
        item_context=item["context"],
        action_keywords=", ".join(item.get("action_keywords", []))
    )
    
    llm_intents = call_llm_for_intents(prompt)
    
    intents = []
    for llm_intent in llm_intents:
        intent_id = f"intent_{intent_id_counter[0]:04d}"
        intent_id_counter[0] += 1
        
        intents.append(Intent(
            intent_id=intent_id,
            clause_id=clause["clause_id"],
            item_no=item["item_no"],
            subitem_no=None,
            user_query=llm_intent.get("user_query", ""),
            excerpt=llm_intent.get("excerpt", ""),
            conditions=llm_intent.get("conditions", []),
            exceptions=llm_intent.get("exceptions", []),
            referenced_clauses=llm_intent.get("referenced_clauses", []),
            category=llm_intent.get("category", "å…¶ä»–"),
            query_type=llm_intent.get("query_type", "ç›´æ¥æŸ¥è©¢"),
            semantic_tags=llm_intent.get("semantic_tags", []),
            difficulty=llm_intent.get("difficulty", "ç°¡å–®")
        ))
    
    return intents


def generate_all_intents(chunks: List[Dict], 
                         generate_for_items: bool = True,
                         generate_comparisons: bool = True) -> List[Intent]:
    """ç”Ÿæˆæ‰€æœ‰æ„åœ–ï¼ˆV2 å¢å¼·ç‰ˆï¼‰"""
    all_intents = []
    intent_id_counter = [1]
    
    total_chunks = len(chunks)
    
    # 1. ç”Ÿæˆæ¢æ–‡å’Œé …ç›®æ„åœ–
    for i, chunk in enumerate(chunks, 1):
        clause = chunk["clause"]
        print(f"ğŸ”„ è™•ç† [{i}/{total_chunks}]: {clause['clause_no']} {clause['clause_title']}")
        
        # æ¢æ–‡ç´šåˆ¥æ„åœ–
        clause_intents = generate_clause_intents(chunk, intent_id_counter)
        all_intents.extend(clause_intents)
        
        # é …ç›®ç´šåˆ¥æ„åœ–
        if generate_for_items and clause.get("items"):
            for item in clause["items"]:
                item_intents = generate_item_intents(chunk, item, intent_id_counter)
                all_intents.extend(item_intents)
                item["intent_ids"] = [intent.intent_id for intent in item_intents]
        
        clause["intent_ids"] = [intent.intent_id for intent in clause_intents]
        
        time.sleep(0.5)
    
    # 2. ç”Ÿæˆå°æ¯”æ„åœ–
    if generate_comparisons:
        print("\nğŸ”„ ç”Ÿæˆå°æ¯”æ„åœ–...")
        comparison_intents = generate_comparison_intents(chunks, intent_id_counter)
        all_intents.extend(comparison_intents)
    
    print(f"\nâœ… ç¸½å…±ç”Ÿæˆ {len(all_intents)} å€‹æ„åœ–")
    
    # çµ±è¨ˆ
    query_types = {}
    for intent in all_intents:
        qt = intent.query_type
        query_types[qt] = query_types.get(qt, 0) + 1
    
    print(f"\nğŸ“Š æ„åœ–é¡å‹åˆ†å¸ƒ:")
    for qt, count in sorted(query_types.items(), key=lambda x: x[1], reverse=True):
        print(f"   - {qt}: {count}")
    
    return all_intents


# ==================== ä¿å­˜å‡½æ•¸ ====================

def save_intents(intents: List[Intent], output_path: str):
    """ä¿å­˜æ„åœ–åˆ° JSON æ–‡ä»¶"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    data = {
        "metadata": {
            "total_intents": len(intents),
            "generated_at": __import__('datetime').datetime.now().isoformat(),
            "version": "2.0",
            "enhancements": [
                "é‡å°æ€§å•æ³•ï¼ˆå€åˆ†å»¶èª¤/éºå¤±/æå¤±ï¼‰",
                "å°æ¯”å¼æ„åœ–",
                "å¤šæ¨£åŒ–å•æ³•",
                "è² å‘æ„åœ–",
                "èªç¾©æ¨™ç±¤"
            ]
        },
        "intents": [intent.to_dict() for intent in intents]
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜ {len(intents)} å€‹æ„åœ–è‡³ {output_path}")


def save_chunks_with_intents(chunks: List[Dict], output_path: str):
    """ä¿å­˜åŒ…å«æ„åœ– ID çš„ chunks"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    data = {
        "metadata": {
            "total_chunks": len(chunks),
            "generated_at": __import__('datetime').datetime.now().isoformat(),
            "version": "2.0"
        },
        "chunks": chunks
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜ chunksï¼ˆV2ï¼Œå«æ„åœ–ï¼‰è‡³ {output_path}")


# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    print("ğŸ”„ é–‹å§‹ç”Ÿæˆæ„åœ–ï¼ˆV2 å¢å¼·ç‰ˆï¼‰...")
    
    # è¼‰å…¥ V2 chunks
    chunks_path = os.path.join(INDEX_DIR, "chunks_structured_v2.json")
    
    if not os.path.exists(chunks_path):
        print(f"âŒ æ‰¾ä¸åˆ° V2 chunks æ–‡ä»¶: {chunks_path}")
        print("è«‹å…ˆé‹è¡Œ chunk_generator_v2.py")
        exit(1)
    
    with open(chunks_path, "r", encoding="utf-8") as f:
        chunks_data = json.load(f)
    
    chunks = chunks_data["chunks"]
    print(f"ğŸ“¥ å·²è¼‰å…¥ {len(chunks)} å€‹ chunksï¼ˆV2ï¼‰")
    
    # ç”Ÿæˆæ„åœ–
    intents = generate_all_intents(
        chunks, 
        generate_for_items=True,
        generate_comparisons=True  # å•Ÿç”¨å°æ¯”æ„åœ–
    )
    
    # ä¿å­˜
    intents_path = os.path.join(INDEX_DIR, "intents_v2.json")
    save_intents(intents, intents_path)
    
    chunks_with_intents_path = os.path.join(INDEX_DIR, "chunks_structured_with_intents_v2.json")
    save_chunks_with_intents(chunks, chunks_with_intents_path)
    
    # è©³ç´°çµ±è¨ˆ
    categories = {}
    semantic_tags_count = {}
    
    for intent in intents:
        cat = intent.category
        categories[cat] = categories.get(cat, 0) + 1
        
        for tag in intent.semantic_tags:
            semantic_tags_count[tag] = semantic_tags_count.get(tag, 0) + 1
    
    print("\nğŸ“Š æ„åœ–åˆ†é¡çµ±è¨ˆ:")
    for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
        print(f"   - {cat}: {count}")
    
    print("\nğŸ·ï¸  èªç¾©æ¨™ç±¤çµ±è¨ˆ:")
    for tag, count in sorted(semantic_tags_count.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"   - {tag}: {count}")

"""
Chunk Generator - å¾ PDF ç”Ÿæˆçµæ§‹åŒ–æ¢æ–‡

ä¸»è¦åŠŸèƒ½:
1. è§£æ PDF æ–‡æœ¬ï¼Œè­˜åˆ¥ç« ç¯€ã€æ¢æ–‡ã€é …ç›®ã€æ¬¾é …çµæ§‹
2. æå–å¼•ç”¨é—œä¿‚ï¼ˆå¦‚ï¼šç¬¬äºŒåä¸ƒæ¢ç¬¬ä¸€é …ç¬¬äºŒæ¬¾ï¼‰
3. ç”Ÿæˆéšå±¤å¼çµæ§‹åŒ– chunks
4. ä¿ç•™å®Œæ•´çš„ context å’Œæ¸…ç†å¾Œçš„ raw_text
"""

import re
import json
import os
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from load_pdf import load_pdf
from config import INDEX_DIR


# ==================== æ­£å‰‡è¡¨é”å¼æ¨¡å¼ ====================

# ç« ç¯€åŒ¹é…ï¼šç¬¬ä¸€ç«  ç¸½å‰‡
CHAPTER_PATTERN = re.compile(r"^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« )\s*(.+?)$", re.MULTILINE)

# æ¢æ–‡åŒ¹é…ï¼šç¬¬ä¸€æ¢ å¥‘ç´„ä¹‹æ§‹æˆ
CLAUSE_PATTERN = re.compile(r"^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¢)\s*(.+?)\s*$", re.MULTILINE)

# é …ç›®åŒ¹é…ï¼šä¸€ã€äºŒã€ä¸‰ã€
ITEM_PATTERN = re.compile(r"^\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)ã€", re.MULTILINE)

# æ¬¾é …åŒ¹é…ï¼š(ä¸€) æˆ– ï¼ˆ1ï¼‰ æˆ– 1) æˆ– (a)
SUBITEM_PATTERN = re.compile(
    r"^\s*(?:\(|ï¼ˆ)([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+|[0-9]+|[a-zA-Z])(?:\)|ï¼‰)\s*",
    re.MULTILINE
)

# å¼•ç”¨é—œä¿‚åŒ¹é…
# åŒ¹é…ï¼šå‰é …ã€å‰æ¬¾ã€ç¬¬XXæ¢ã€ç¬¬XXæ¢ç¬¬Xé …ã€ç¬¬XXæ¢ç¬¬Xé …ç¬¬Xæ¬¾
REFERENCE_PATTERN = re.compile(
    r"(å‰é …|å‰æ¬¾|æœ¬æ¢|"
    r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¢(?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+é …)?(?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¬¾)?)",
    re.MULTILINE
)


# ==================== æ•¸æ“šçµæ§‹ ====================

@dataclass
class SubItem:
    """æ¬¾é …çµæ§‹"""
    subitem_no: str  # (ä¸€)ã€(1)ã€(a) ç­‰
    context: str  # å®Œæ•´æ–‡æœ¬ï¼ˆä¿ç•™æ ¼å¼ï¼‰
    raw_text: str  # æ¸…ç†å¾Œæ–‡æœ¬ï¼ˆå»é™¤ç·¨è™Ÿï¼‰
    reference_clauses: List[str]  # å¼•ç”¨çš„æ¢æ–‡
    
    def to_dict(self):
        return asdict(self)


@dataclass
class Item:
    """é …ç›®çµæ§‹"""
    item_no: str  # ä¸€ã€äºŒã€ä¸‰
    context: str  # å®Œæ•´æ–‡æœ¬
    raw_text: str  # æ¸…ç†å¾Œæ–‡æœ¬
    sub_items: List[SubItem]  # å­æ¬¾é …
    reference_clauses: List[str]  # å¼•ç”¨çš„æ¢æ–‡
    intent_ids: List[str]  # é—œè¯çš„æ„åœ– IDï¼ˆå¾ŒçºŒå¡«å……ï¼‰
    
    def to_dict(self):
        return {
            "item_no": self.item_no,
            "context": self.context,
            "raw_text": self.raw_text,
            "sub_items": [si.to_dict() for si in self.sub_items],
            "reference_clauses": self.reference_clauses,
            "intent_ids": self.intent_ids
        }


@dataclass
class Clause:
    """æ¢æ–‡ä¸»é«”çµæ§‹"""
    clause_no: str  # ç¬¬ä¸€æ¢
    clause_title: str  # å¥‘ç´„ä¹‹æ§‹æˆ
    clause_id: str  # ç¬¬ä¸€æ¢_å¥‘ç´„ä¹‹æ§‹æˆ
    context: str  # å®Œæ•´æ¢æ–‡å…§å®¹
    raw_text: str  # æ¸…ç†å¾Œå…§å®¹
    items: List[Item]  # å­é …ç›®åˆ—è¡¨
    reference_clauses: List[str]  # å¼•ç”¨çš„æ¢æ–‡
    intent_ids: List[str]  # é—œè¯çš„æ„åœ– ID
    
    def to_dict(self):
        return {
            "clause_no": self.clause_no,
            "clause_title": self.clause_title,
            "clause_id": self.clause_id,
            "context": self.context,
            "raw_text": self.raw_text,
            "items": [item.to_dict() for item in self.items],
            "reference_clauses": self.reference_clauses,
            "intent_ids": self.intent_ids
        }


@dataclass
class Chunk:
    """å®Œæ•´çš„ chunk çµæ§‹ï¼ˆåŒ…å«ç« ç¯€ä¿¡æ¯ï¼‰"""
    chunk_id: str  # èˆ‡ clause_id ç›¸åŒ
    chapter_no: Optional[str]  # ç¬¬ä¸€ç« 
    chapter_title: Optional[str]  # ç¸½å‰‡
    clause: Clause  # æ¢æ–‡ä¸»é«”
    
    def to_dict(self):
        return {
            "chunk_id": self.chunk_id,
            "chapter_no": self.chapter_no,
            "chapter_title": self.chapter_title,
            "clause": self.clause.to_dict(),
            # ç‚ºäº†å‘å¾Œå…¼å®¹ï¼Œä¿ç•™é ‚å±¤å­—æ®µ
            "context": self.clause.context,
            "raw_text": self.clause.raw_text,
            "items": [item.to_dict() for item in self.clause.items],
            "intent_ids": self.clause.intent_ids,
            "reference_clauses": self.clause.reference_clauses
        }


# ==================== è¼”åŠ©å‡½æ•¸ ====================

def clean_text(text: str, remove_numbers: bool = True) -> str:
    """
    æ¸…ç†æ–‡æœ¬
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        remove_numbers: æ˜¯å¦ç§»é™¤ç·¨è™Ÿï¼ˆä¸€ã€(ä¸€)ã€1) ç­‰ï¼‰
    
    Returns:
        æ¸…ç†å¾Œçš„æ–‡æœ¬
    """
    # ç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œ
    text = re.sub(r'\s+', ' ', text).strip()
    
    if remove_numbers:
        # ç§»é™¤é …ç›®ç·¨è™Ÿï¼šä¸€ã€äºŒã€ä¸‰ã€
        text = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ã€\s*', '', text)
        # ç§»é™¤æ¬¾é …ç·¨è™Ÿï¼š(ä¸€)ã€ï¼ˆ1ï¼‰ã€1)ã€(a)
        text = re.sub(r'^(?:\(|ï¼ˆ)[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾0-9a-zA-Z]+(?:\)|ï¼‰)\s*', '', text)
    
    return text


def detect_reference_clauses(text: str) -> List[str]:
    """
    æª¢æ¸¬æ–‡æœ¬ä¸­çš„å¼•ç”¨é—œä¿‚
    
    Returns:
        å¼•ç”¨çš„æ¢æ–‡åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š['å‰é …', 'ç¬¬äºŒåä¸ƒæ¢', 'ç¬¬äºŒåä¸ƒæ¢ç¬¬ä¸€é …ç¬¬äºŒæ¬¾']
    """
    matches = REFERENCE_PATTERN.findall(text)
    # å»é‡ä¸¦ä¿æŒé †åº
    seen = set()
    result = []
    for ref in matches:
        if ref not in seen:
            seen.add(ref)
            result.append(ref)
    return result


def parse_subitems(text: str) -> List[SubItem]:
    """
    è§£ææ¬¾é …ï¼ˆç¬¬ä¸‰å±¤ï¼‰
    
    Args:
        text: åŒ…å«æ¬¾é …çš„æ–‡æœ¬
    
    Returns:
        SubItem åˆ—è¡¨
    """
    matches = list(SUBITEM_PATTERN.finditer(text))
    if not matches:
        return []
    
    subitems = []
    for i, match in enumerate(matches):
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        
        subitem_text = text[start:end].strip()
        
        subitems.append(SubItem(
            subitem_no=match.group(1),
            context=subitem_text,
            raw_text=clean_text(subitem_text, remove_numbers=True),
            reference_clauses=detect_reference_clauses(subitem_text)
        ))
    
    return subitems


def parse_items(text: str) -> List[Item]:
    """
    è§£æé …ç›®ï¼ˆç¬¬äºŒå±¤ï¼‰
    
    Args:
        text: åŒ…å«é …ç›®çš„æ–‡æœ¬
    
    Returns:
        Item åˆ—è¡¨
    """
    matches = list(ITEM_PATTERN.finditer(text))
    if not matches:
        return []
    
    items = []
    for i, match in enumerate(matches):
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        
        item_text = text[start:end].strip()
        
        # è§£æå­æ¬¾é …
        sub_items = parse_subitems(item_text)
        
        items.append(Item(
            item_no=match.group(1),
            context=item_text,
            raw_text=clean_text(item_text, remove_numbers=True),
            sub_items=sub_items,
            reference_clauses=detect_reference_clauses(item_text),
            intent_ids=[]  # ç¨å¾Œå¡«å……
        ))
    
    return items


def extract_chapter_info(text: str, clause_start: int) -> Tuple[Optional[str], Optional[str]]:
    """
    æå–æ¢æ–‡æ‰€å±¬çš„ç« ç¯€ä¿¡æ¯
    
    Args:
        text: å®Œæ•´æ–‡æœ¬
        clause_start: æ¢æ–‡åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®
    
    Returns:
        (ç« ç¯€ç·¨è™Ÿ, ç« ç¯€æ¨™é¡Œ) æˆ– (None, None)
    """
    # æ‰¾åˆ°æ‰€æœ‰ç« ç¯€
    chapters = list(CHAPTER_PATTERN.finditer(text))
    
    # æ‰¾åˆ°æœ€æ¥è¿‘çš„å‰ä¸€å€‹ç« ç¯€
    current_chapter_no = None
    current_chapter_title = None
    
    for chapter_match in chapters:
        if chapter_match.start() < clause_start:
            current_chapter_no = chapter_match.group(1)
            current_chapter_title = chapter_match.group(2).strip()
        else:
            break
    
    return current_chapter_no, current_chapter_title


# ==================== ä¸»è¦å‡½æ•¸ ====================

def generate_chunks_from_pdf(pdf_path: Optional[str] = None) -> List[Chunk]:
    """
    å¾ PDF ç”Ÿæˆçµæ§‹åŒ– chunks
    
    Args:
        pdf_path: PDF è·¯å¾‘ï¼ˆå¯é¸ï¼Œä½¿ç”¨ load_pdf çš„é»˜èªè·¯å¾‘ï¼‰
    
    Returns:
        Chunk åˆ—è¡¨
    """
    # è¼‰å…¥ PDF æ–‡æœ¬
    text = load_pdf().strip()
    
    # æ‰¾åˆ°æ‰€æœ‰æ¢æ–‡
    clause_matches = list(CLAUSE_PATTERN.finditer(text))
    
    chunks = []
    
    for i, clause_match in enumerate(clause_matches):
        clause_no = clause_match.group(1)
        clause_title = clause_match.group(2).strip()
        clause_id = f"{clause_no}_{clause_title}"
        
        # æå–æ¢æ–‡å…§å®¹
        clause_start = clause_match.end()
        clause_end = clause_matches[i + 1].start() if i + 1 < len(clause_matches) else len(text)
        clause_body = text[clause_start:clause_end].strip()
        
        # æå–ç« ç¯€ä¿¡æ¯
        chapter_no, chapter_title = extract_chapter_info(text, clause_match.start())
        
        # è§£æé …ç›®
        items = parse_items(clause_body)
        
        # æ§‹å»ºæ¢æ–‡å°è±¡
        clause = Clause(
            clause_no=clause_no,
            clause_title=clause_title,
            clause_id=clause_id,
            context=clause_body,
            raw_text=clean_text(clause_body, remove_numbers=True),
            items=items,
            reference_clauses=detect_reference_clauses(clause_body),
            intent_ids=[]
        )
        
        # æ§‹å»º chunk
        chunk = Chunk(
            chunk_id=clause_id,
            chapter_no=chapter_no,
            chapter_title=chapter_title,
            clause=clause
        )
        
        chunks.append(chunk)
    
    return chunks


def save_chunks(chunks: List[Chunk], output_path: str):
    """
    ä¿å­˜ chunks åˆ° JSON æ–‡ä»¶
    
    Args:
        chunks: Chunk åˆ—è¡¨
        output_path: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    data = {
        "metadata": {
            "total_chunks": len(chunks),
            "generated_at": __import__('datetime').datetime.now().isoformat()
        },
        "chunks": [chunk.to_dict() for chunk in chunks]
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ç”Ÿæˆ {len(chunks)} å€‹ chunksï¼Œä¿å­˜è‡³ {output_path}")


# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    print("ğŸ”„ é–‹å§‹ç”Ÿæˆ chunks...")
    
    # ç”Ÿæˆ chunks
    chunks = generate_chunks_from_pdf()
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_path = os.path.join(INDEX_DIR, "chunks_structured.json")
    save_chunks(chunks, output_path)
    
    # æ‰“å°çµ±è¨ˆä¿¡æ¯
    total_items = sum(len(chunk.clause.items) for chunk in chunks)
    total_subitems = sum(
        len(item.sub_items) 
        for chunk in chunks 
        for item in chunk.clause.items
    )
    
    print(f"ğŸ“Š çµ±è¨ˆ:")
    print(f"   - ç¸½æ¢æ–‡æ•¸: {len(chunks)}")
    print(f"   - ç¸½é …ç›®æ•¸: {total_items}")
    print(f"   - ç¸½æ¬¾é …æ•¸: {total_subitems}")
    
    # é¡¯ç¤ºç¬¬ä¸€å€‹ chunk ç¤ºä¾‹
    if chunks:
        print("\nğŸ“„ ç¬¬ä¸€å€‹ chunk ç¤ºä¾‹:")
        print(json.dumps(chunks[0].to_dict(), ensure_ascii=False, indent=2)[:500] + "...")

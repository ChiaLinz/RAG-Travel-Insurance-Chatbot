"""
Chunk Generator V2 - å„ªåŒ–ç‰ˆæ¢æ–‡è§£æå™¨

ä¸»è¦å„ªåŒ–ï¼š
1. æ›´ç²¾ç´°çš„èªç¾©æ¨™è¨»ï¼ˆå€åˆ†ã€Œå»¶èª¤ã€vsã€Œéºå¤±ã€vsã€Œæå¤±ã€ï¼‰
2. å¢å¼·çš„å¼•ç”¨é—œä¿‚è§£æï¼ˆæ”¯æ´ç›¸å°å¼•ç”¨ï¼šå‰é …ã€å‰æ¬¾ï¼‰
3. é—œéµè©æå–ï¼ˆè‡ªå‹•æ¨™è¨»æ¯å€‹ chunk çš„æ ¸å¿ƒæ¦‚å¿µï¼‰
4. æ¢æ–‡åˆ†é¡ï¼ˆè‡ªå‹•è­˜åˆ¥ä¿éšªé¡å‹ï¼‰
"""

import re
import json
import os
from typing import List, Dict, Optional, Tuple, Set
from dataclasses import dataclass, asdict, field
from load_pdf import load_pdf
from config import INDEX_DIR


# ==================== æ­£å‰‡è¡¨é”å¼æ¨¡å¼ ====================

# ç« ç¯€åŒ¹é…ï¼šç¬¬ä¸€ç«  ç¸½å‰‡
CHAPTER_PATTERN = re.compile(r"^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« )\s*(.+?)$", re.MULTILINE)

# æ¢æ–‡åŒ¹é…ï¼šç¬¬ä¸€æ¢ å¥‘ç´„ä¹‹æ§‹æˆ
CLAUSE_PATTERN = re.compile(r"^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¢)\s*(.+?)\s*$", re.MULTILINE)

# é …ç›®åŒ¹é…ï¼šä¸€ã€äºŒã€ä¸‰ã€
ITEM_PATTERN = re.compile(r"^\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)ã€", re.MULTILINE)

# æ¬¾é …åŒ¹é…ï¼š(ä¸€) æˆ– ï¼ˆ1ï¼‰ æˆ– 1) æˆ– (a)
SUBITEM_PATTERN = re.compile(
    r"^\s*(?:\(|ï¼ˆ)([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+|[0-9]+|[a-zA-Z])(?:\)|ï¼‰)\s*",
    re.MULTILINE
)

# å¼•ç”¨é—œä¿‚åŒ¹é… - å¢å¼·ç‰ˆ
REFERENCE_PATTERN = re.compile(
    r"(å‰é …|å‰æ¬¾|å‰æ¢|æœ¬æ¢|æœ¬é …|æœ¬æ¬¾|"
    r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¢(?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+é …)?(?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¬¾)?)",
    re.MULTILINE
)


# ==================== èªç¾©åˆ†é¡å™¨ ====================

class SemanticClassifier:
    """èªç¾©åˆ†é¡å™¨ - è‡ªå‹•è­˜åˆ¥æ¢æ–‡é¡å‹å’Œé—œéµæ¦‚å¿µ"""
    
    # ä¿éšªé¡å‹é—œéµè©
    INSURANCE_TYPES = {
        "ç­æ©Ÿå»¶èª¤": ["ç­æ©Ÿ", "èˆªç­", "å®šæœŸèˆªç­", "å»¶èª¤", "é å®šå‡ºç™¼æ™‚é–“"],
        "è¡Œæå»¶èª¤": ["è¡Œæ", "å»¶èª¤", "æŠµé”ç›®çš„åœ°", "æœªé ˜å¾—"],
        "è¡Œææå¤±": ["è¡Œæ", "æå¤±", "æ¯€æ", "æ»…å¤±", "éºå¤±", "ç«Šç›œ", "å¼·ç›œ", "æ¶å¥ª"],
        "æ—…ç¨‹å–æ¶ˆ": ["æ—…ç¨‹", "å–æ¶ˆ", "é å®š", "ç„¡æ³•å–å›"],
        "æ—…ç¨‹æ›´æ”¹": ["æ—…ç¨‹", "æ›´æ”¹", "å¢åŠ ä¹‹äº¤é€š", "ä½å®¿è²»ç”¨"],
        "ç§Ÿè»Šäº‹æ•…": ["ç§Ÿç”¨æ±½è»Š", "ç§Ÿè»Š", "é§•é§›", "äº¤é€šäº‹æ•…"],
        "ç¾é‡‘ç«Šç›œ": ["ç¾é‡‘", "ç«Šç›œ", "å¼·ç›œ", "æ¶å¥ª", "éš¨èº«æ”œå¸¶"],
        "ä¿¡ç”¨å¡ç›œç”¨": ["ä¿¡ç”¨å¡", "ç›œç”¨", "æ›å¤±", "æ­¢ä»˜"],
        "æ€¥é›£æ•‘åŠ©": ["æ€¥é›£", "æ•‘åŠ©", "è½‰é€", "æœç´¢", "æ•‘æ´"],
    }
    
    # æ¢æ–‡åŠŸèƒ½é¡å‹
    CLAUSE_FUNCTIONS = {
        "æ‰¿ä¿ç¯„åœ": ["æ‰¿ä¿ç¯„åœ", "ä¿éšªç¯„åœ", "æœ¬å…¬å¸ä¾æœ¬ä¿éšªå¥‘ç´„"],
        "ä¸ä¿äº‹é …": ["ä¸ä¿äº‹é …", "ä¸è² ç†è³ è²¬ä»»", "é™¤å¤–è²¬ä»»", "ç‰¹åˆ¥ä¸ä¿"],
        "ç†è³ æ–‡ä»¶": ["ç†è³ æ–‡ä»¶", "ç”³è«‹ç†è³ ", "æ‡‰æª¢å…·ä¸‹åˆ—æ–‡ä»¶"],
        "ç†è³ é‡‘é¡": ["ä¿éšªé‡‘é¡", "çµ¦ä»˜", "ç†è³ é‡‘é¡", "æœ€é«˜ä»¥"],
        "å®šç¾©èªªæ˜": ["æ‰€ç¨±", "ä¿‚æŒ‡", "å®šç¾©"],
    }
    
    # é—œéµå‹•ä½œè©ï¼ˆç”¨æ–¼å€åˆ†ç›¸ä¼¼æ¦‚å¿µï¼‰
    ACTION_KEYWORDS = {
        "å»¶èª¤": ["å»¶èª¤", "å»¶é²", "æœªæ–¼", "è¶…éæ™‚é–“"],
        "éºå¤±": ["éºå¤±", "å¤±è¹¤", "æœªå°‹ç²"],
        "æå¤±": ["æå¤±", "æ¯€æ", "æ»…å¤±"],
        "å–æ¶ˆ": ["å–æ¶ˆ", "çµ‚æ­¢", "ä¸­æ­¢"],
        "æ›´æ”¹": ["æ›´æ”¹", "è®Šæ›´", "èª¿æ•´"],
        "ç«Šç›œ": ["ç«Šç›œ", "å·ç«Š"],
        "æ¶å¥ª": ["æ¶å¥ª", "æ¶åŠ«", "å¼·ç›œ"],
    }
    
    @staticmethod
    def classify_insurance_type(text: str, clause_title: str) -> List[str]:
        """è­˜åˆ¥ä¿éšªé¡å‹"""
        types = []
        combined_text = f"{clause_title} {text}"
        
        for insurance_type, keywords in SemanticClassifier.INSURANCE_TYPES.items():
            if any(kw in combined_text for kw in keywords):
                types.append(insurance_type)
        
        return types if types else ["å…¶ä»–"]
    
    @staticmethod
    def classify_clause_function(text: str, clause_title: str) -> str:
        """è­˜åˆ¥æ¢æ–‡åŠŸèƒ½"""
        combined_text = f"{clause_title} {text}"
        
        for func_type, keywords in SemanticClassifier.CLAUSE_FUNCTIONS.items():
            if any(kw in combined_text for kw in keywords):
                return func_type
        
        return "ä¸€èˆ¬è¦å®š"
    
    @staticmethod
    def extract_action_keywords(text: str) -> List[str]:
        """æå–å‹•ä½œé—œéµè©"""
        actions = []
        
        for action, keywords in SemanticClassifier.ACTION_KEYWORDS.items():
            if any(kw in text for kw in keywords):
                actions.append(action)
        
        return actions
    
    @staticmethod
    def extract_entities(text: str) -> Dict[str, List[str]]:
        """æå–å¯¦é«”ï¼ˆæ™‚é–“ã€é‡‘é¡ã€æ•¸é‡ç­‰ï¼‰"""
        entities = {
            "æ™‚é–“": [],
            "é‡‘é¡": [],
            "æ¬¡æ•¸": [],
            "åœ°é»": []
        }
        
        # æ™‚é–“å¯¦é«”
        time_patterns = [
            r"(\d+(?:å°æ™‚|å¤©|æ—¥|å€‹æœˆ|å¹´))",
            r"(äºŒåå››å°æ™‚|å››å°æ™‚|å…­å°æ™‚)",
            r"(é å®šå‡ºç™¼æ™‚é–“|å¯¦éš›å‡ºç™¼æ™‚é–“)"
        ]
        for pattern in time_patterns:
            matches = re.findall(pattern, text)
            entities["æ™‚é–“"].extend(matches)
        
        # é‡‘é¡å¯¦é«”
        money_patterns = [
            r"(æ–°è‡ºå¹£\s*[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬å„„\d]+å…ƒ)",
            r"(ä¿éšªé‡‘é¡)",
        ]
        for pattern in money_patterns:
            matches = re.findall(pattern, text)
            entities["é‡‘é¡"].extend(matches)
        
        # æ¬¡æ•¸å¯¦é«”
        count_patterns = [
            r"(çµ¦ä»˜[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ¬¡)",
            r"([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ¬¡äº‹æ•…)",
        ]
        for pattern in count_patterns:
            matches = re.findall(pattern, text)
            entities["æ¬¡æ•¸"].extend(matches)
        
        # å»é‡
        for key in entities:
            entities[key] = list(set(entities[key]))
        
        return entities


# ==================== å¼•ç”¨è§£æå™¨ ====================

class ReferenceResolver:
    """å¼•ç”¨è§£æå™¨ - è™•ç†ç›¸å°å’Œçµ•å°å¼•ç”¨"""
    
    def __init__(self, chunks: List[Dict]):
        """åˆå§‹åŒ–è§£æå™¨"""
        self.chunks = chunks
        self.clause_index = {}  # clause_no -> index
        
        # å»ºç«‹ç´¢å¼•
        for i, chunk in enumerate(chunks):
            clause_no = chunk["clause"]["clause_no"]
            self.clause_index[clause_no] = i
    
    def resolve_reference(self, 
                         ref: str, 
                         current_clause_no: str,
                         current_item_no: Optional[str] = None) -> str:
        """
        è§£æå¼•ç”¨é—œä¿‚
        
        Args:
            ref: å¼•ç”¨æ–‡æœ¬ï¼ˆå¦‚ï¼š"å‰é …"ã€"ç¬¬äºŒåä¸ƒæ¢ç¬¬ä¸€é …"ï¼‰
            current_clause_no: ç•¶å‰æ¢æ–‡ç·¨è™Ÿ
            current_item_no: ç•¶å‰é …ç›®ç·¨è™Ÿï¼ˆå¯é¸ï¼‰
        
        Returns:
            è§£æå¾Œçš„çµ•å°å¼•ç”¨ï¼ˆå¦‚ï¼š"ç¬¬äºŒåå…­æ¢ç¬¬äºŒé …"ï¼‰
        """
        # çµ•å°å¼•ç”¨ï¼ˆå·²ç¶“æ˜¯å®Œæ•´çš„ï¼‰
        if ref.startswith("ç¬¬") and "æ¢" in ref:
            return ref
        
        # ç›¸å°å¼•ç”¨
        if ref == "å‰é …":
            # éœ€è¦æ‰¾åˆ°å‰ä¸€é …
            if current_item_no:
                prev_item = self._get_previous_item(current_clause_no, current_item_no)
                if prev_item:
                    return f"{current_clause_no}ç¬¬{prev_item}é …"
            return f"{current_clause_no}å‰é …"
        
        elif ref == "å‰æ¬¾":
            # éœ€è¦æ‰¾åˆ°å‰ä¸€æ¬¾
            return f"{current_clause_no}å‰æ¬¾"
        
        elif ref == "å‰æ¢":
            prev_clause = self._get_previous_clause(current_clause_no)
            if prev_clause:
                return prev_clause
            return ref
        
        elif ref in ["æœ¬æ¢", "æœ¬é …", "æœ¬æ¬¾"]:
            return f"{current_clause_no}{ref[1:]}"
        
        return ref
    
    def _get_previous_clause(self, current_clause_no: str) -> Optional[str]:
        """ç²å–å‰ä¸€æ¢æ–‡"""
        # æå–æ•¸å­—
        match = re.search(r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)æ¢", current_clause_no)
        if not match:
            return None
        
        # TODO: å¯¦ç¾ä¸­æ–‡æ•¸å­—è½‰æ›
        return None
    
    def _get_previous_item(self, clause_no: str, current_item_no: str) -> Optional[str]:
        """ç²å–å‰ä¸€é …"""
        # ç°¡å–®æ˜ å°„
        item_map = {
            "äºŒ": "ä¸€", "ä¸‰": "äºŒ", "å››": "ä¸‰", "äº”": "å››",
            "å…­": "äº”", "ä¸ƒ": "å…­", "å…«": "ä¸ƒ", "ä¹": "å…«", "å": "ä¹"
        }
        return item_map.get(current_item_no)


# ==================== æ•¸æ“šçµæ§‹ ====================

@dataclass
class SubItem:
    """æ¬¾é …çµæ§‹"""
    subitem_no: str
    context: str
    raw_text: str
    reference_clauses: List[str]
    
    # æ–°å¢å­—æ®µ
    action_keywords: List[str] = field(default_factory=list)
    entities: Dict[str, List[str]] = field(default_factory=dict)
    
    def to_dict(self):
        return asdict(self)


@dataclass
class Item:
    """é …ç›®çµæ§‹"""
    item_no: str
    context: str
    raw_text: str
    sub_items: List[SubItem]
    reference_clauses: List[str]
    intent_ids: List[str]
    
    # æ–°å¢å­—æ®µ
    action_keywords: List[str] = field(default_factory=list)
    entities: Dict[str, List[str]] = field(default_factory=dict)
    
    def to_dict(self):
        return {
            "item_no": self.item_no,
            "context": self.context,
            "raw_text": self.raw_text,
            "sub_items": [si.to_dict() for si in self.sub_items],
            "reference_clauses": self.reference_clauses,
            "intent_ids": self.intent_ids,
            "action_keywords": self.action_keywords,
            "entities": self.entities
        }


@dataclass
class Clause:
    """æ¢æ–‡ä¸»é«”çµæ§‹"""
    clause_no: str
    clause_title: str
    clause_id: str
    context: str
    raw_text: str
    items: List[Item]
    reference_clauses: List[str]
    intent_ids: List[str]
    
    # æ–°å¢å­—æ®µ
    insurance_types: List[str] = field(default_factory=list)
    clause_function: str = ""
    action_keywords: List[str] = field(default_factory=list)
    entities: Dict[str, List[str]] = field(default_factory=dict)
    
    def to_dict(self):
        return {
            "clause_no": self.clause_no,
            "clause_title": self.clause_title,
            "clause_id": self.clause_id,
            "context": self.context,
            "raw_text": self.raw_text,
            "items": [item.to_dict() for item in self.items],
            "reference_clauses": self.reference_clauses,
            "intent_ids": self.intent_ids,
            "insurance_types": self.insurance_types,
            "clause_function": self.clause_function,
            "action_keywords": self.action_keywords,
            "entities": self.entities
        }


@dataclass
class Chunk:
    """å®Œæ•´çš„ chunk çµæ§‹"""
    chunk_id: str
    chapter_no: Optional[str]
    chapter_title: Optional[str]
    clause: Clause
    
    def to_dict(self):
        return {
            "chunk_id": self.chunk_id,
            "chapter_no": self.chapter_no,
            "chapter_title": self.chapter_title,
            "clause": self.clause.to_dict(),
            # å‘å¾Œå…¼å®¹
            "context": self.clause.context,
            "raw_text": self.clause.raw_text,
            "items": [item.to_dict() for item in self.clause.items],
            "intent_ids": self.clause.intent_ids,
            "reference_clauses": self.clause.reference_clauses
        }


# ==================== è¼”åŠ©å‡½æ•¸ ====================

def clean_text(text: str, remove_numbers: bool = True) -> str:
    """æ¸…ç†æ–‡æœ¬"""
    text = re.sub(r'\s+', ' ', text).strip()
    
    if remove_numbers:
        text = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ã€\s*', '', text)
        text = re.sub(r'^(?:\(|ï¼ˆ)[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾0-9a-zA-Z]+(?:\)|ï¼‰)\s*', '', text)
    
    return text


def detect_reference_clauses(text: str) -> List[str]:
    """æª¢æ¸¬æ–‡æœ¬ä¸­çš„å¼•ç”¨é—œä¿‚"""
    matches = REFERENCE_PATTERN.findall(text)
    seen = set()
    result = []
    for ref in matches:
        if ref not in seen:
            seen.add(ref)
            result.append(ref)
    return result


def parse_subitems(text: str, clause_no: str, item_no: str) -> List[SubItem]:
    """è§£ææ¬¾é …"""
    matches = list(SUBITEM_PATTERN.finditer(text))
    if not matches:
        return []
    
    subitems = []
    for i, match in enumerate(matches):
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        
        subitem_text = text[start:end].strip()
        
        # èªç¾©åˆ†æ
        action_keywords = SemanticClassifier.extract_action_keywords(subitem_text)
        entities = SemanticClassifier.extract_entities(subitem_text)
        
        subitems.append(SubItem(
            subitem_no=match.group(1),
            context=subitem_text,
            raw_text=clean_text(subitem_text, remove_numbers=True),
            reference_clauses=detect_reference_clauses(subitem_text),
            action_keywords=action_keywords,
            entities=entities
        ))
    
    return subitems


def parse_items(text: str, clause_no: str, clause_title: str) -> List[Item]:
    """è§£æé …ç›®"""
    matches = list(ITEM_PATTERN.finditer(text))
    if not matches:
        return []
    
    items = []
    for i, match in enumerate(matches):
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        
        item_text = text[start:end].strip()
        item_no = match.group(1)
        
        # è§£æå­æ¬¾é …
        sub_items = parse_subitems(item_text, clause_no, item_no)
        
        # èªç¾©åˆ†æ
        action_keywords = SemanticClassifier.extract_action_keywords(item_text)
        entities = SemanticClassifier.extract_entities(item_text)
        
        items.append(Item(
            item_no=item_no,
            context=item_text,
            raw_text=clean_text(item_text, remove_numbers=True),
            sub_items=sub_items,
            reference_clauses=detect_reference_clauses(item_text),
            intent_ids=[],
            action_keywords=action_keywords,
            entities=entities
        ))
    
    return items


def extract_chapter_info(text: str, clause_start: int) -> Tuple[Optional[str], Optional[str]]:
    """æå–ç« ç¯€ä¿¡æ¯"""
    chapters = list(CHAPTER_PATTERN.finditer(text))
    
    current_chapter_no = None
    current_chapter_title = None
    
    for chapter_match in chapters:
        if chapter_match.start() < clause_start:
            current_chapter_no = chapter_match.group(1)
            current_chapter_title = chapter_match.group(2).strip()
        else:
            break
    
    return current_chapter_no, current_chapter_title


# ==================== ä¸»è¦å‡½æ•¸ ====================

def generate_chunks_from_pdf(pdf_path: Optional[str] = None) -> List[Chunk]:
    """å¾ PDF ç”Ÿæˆçµæ§‹åŒ– chunksï¼ˆV2 å¢å¼·ç‰ˆï¼‰"""
    text = load_pdf().strip()
    
    clause_matches = list(CLAUSE_PATTERN.finditer(text))
    
    chunks = []
    
    for i, clause_match in enumerate(clause_matches):
        clause_no = clause_match.group(1)
        clause_title = clause_match.group(2).strip()
        clause_id = f"{clause_no}_{clause_title}"
        
        # æå–æ¢æ–‡å…§å®¹
        clause_start = clause_match.end()
        clause_end = clause_matches[i + 1].start() if i + 1 < len(clause_matches) else len(text)
        clause_body = text[clause_start:clause_end].strip()
        
        # æå–ç« ç¯€ä¿¡æ¯
        chapter_no, chapter_title = extract_chapter_info(text, clause_match.start())
        
        # è§£æé …ç›®
        items = parse_items(clause_body, clause_no, clause_title)
        
        # èªç¾©åˆ†æ
        insurance_types = SemanticClassifier.classify_insurance_type(clause_body, clause_title)
        clause_function = SemanticClassifier.classify_clause_function(clause_body, clause_title)
        action_keywords = SemanticClassifier.extract_action_keywords(clause_body)
        entities = SemanticClassifier.extract_entities(clause_body)
        
        # æ§‹å»ºæ¢æ–‡å°è±¡
        clause = Clause(
            clause_no=clause_no,
            clause_title=clause_title,
            clause_id=clause_id,
            context=clause_body,
            raw_text=clean_text(clause_body, remove_numbers=True),
            items=items,
            reference_clauses=detect_reference_clauses(clause_body),
            intent_ids=[],
            insurance_types=insurance_types,
            clause_function=clause_function,
            action_keywords=action_keywords,
            entities=entities
        )
        
        # æ§‹å»º chunk
        chunk = Chunk(
            chunk_id=clause_id,
            chapter_no=chapter_no,
            chapter_title=chapter_title,
            clause=clause
        )
        
        chunks.append(chunk)
    
    return chunks


def save_chunks(chunks: List[Chunk], output_path: str):
    """ä¿å­˜ chunks åˆ° JSON æ–‡ä»¶"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    data = {
        "metadata": {
            "total_chunks": len(chunks),
            "generated_at": __import__('datetime').datetime.now().isoformat(),
            "version": "2.0",
            "enhancements": [
                "èªç¾©åˆ†é¡ï¼ˆä¿éšªé¡å‹ã€æ¢æ–‡åŠŸèƒ½ï¼‰",
                "å‹•ä½œé—œéµè©æå–",
                "å¯¦é«”æå–ï¼ˆæ™‚é–“ã€é‡‘é¡ã€æ¬¡æ•¸ï¼‰",
                "å¢å¼·çš„å¼•ç”¨é—œä¿‚"
            ]
        },
        "chunks": [chunk.to_dict() for chunk in chunks]
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ç”Ÿæˆ {len(chunks)} å€‹ chunksï¼ˆV2ï¼‰ï¼Œä¿å­˜è‡³ {output_path}")


# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    print("ğŸ”„ é–‹å§‹ç”Ÿæˆ chunksï¼ˆV2 å¢å¼·ç‰ˆï¼‰...")
    
    chunks = generate_chunks_from_pdf()
    
    output_path = os.path.join(INDEX_DIR, "chunks_structured_v2.json")
    save_chunks(chunks, output_path)
    
    # æ‰“å°çµ±è¨ˆ
    total_items = sum(len(chunk.clause.items) for chunk in chunks)
    insurance_types_count = {}
    clause_functions_count = {}
    
    for chunk in chunks:
        for itype in chunk.clause.insurance_types:
            insurance_types_count[itype] = insurance_types_count.get(itype, 0) + 1
        
        func = chunk.clause.clause_function
        clause_functions_count[func] = clause_functions_count.get(func, 0) + 1
    
    print(f"\nğŸ“Š çµ±è¨ˆ:")
    print(f"   - ç¸½æ¢æ–‡æ•¸: {len(chunks)}")
    print(f"   - ç¸½é …ç›®æ•¸: {total_items}")
    
    print(f"\nğŸ·ï¸  ä¿éšªé¡å‹åˆ†å¸ƒ:")
    for itype, count in sorted(insurance_types_count.items(), key=lambda x: x[1], reverse=True):
        print(f"   - {itype}: {count}")
    
    print(f"\nğŸ“‹ æ¢æ–‡åŠŸèƒ½åˆ†å¸ƒ:")
    for func, count in sorted(clause_functions_count.items(), key=lambda x: x[1], reverse=True):
        print(f"   - {func}: {count}")

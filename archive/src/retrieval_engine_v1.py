"""
Retrieval Engine - RAG æª¢ç´¢å¼•æ“

æª¢ç´¢æµç¨‹ï¼š
1. Query Embedding - ç”¨æˆ¶å•é¡ŒåµŒå…¥
2. Intent Retrieval - æª¢ç´¢ Top-N æ„åœ–
3. Clause Expansion - æ“´å±•ç›¸é—œæ¢æ–‡ï¼ˆæ¯æ¢æ–‡ + å­é …ç›® + è¢«å¼•ç”¨æ¢æ–‡ï¼‰
4. Reranking - ä½¿ç”¨èªç¾©ç›¸ä¼¼åº¦é‡æ’åº
5. Context Building - æ§‹å»ºæœ€çµ‚ä¸Šä¸‹æ–‡
"""

import json
import os
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from config import INDEX_DIR, EMBEDDING_TYPE, OPENAI_EMBEDDING_MODEL, SENTENCE_TRANSFORMER_MODEL


# ==================== åµŒå…¥æ¨¡å‹åˆå§‹åŒ– ====================

class EmbeddingModel:
    """çµ±ä¸€çš„åµŒå…¥æ¨¡å‹æ¥å£"""
    
    def __init__(self):
        self.model_type = EMBEDDING_TYPE
        
        if self.model_type == "openai":
            from openai import OpenAI
            from dotenv import load_dotenv
            load_dotenv()
            self.client = OpenAI()
            self.model_name = OPENAI_EMBEDDING_MODEL
            print(f"ğŸ”„ ä½¿ç”¨ OpenAI Embedding: {self.model_name}")
            
        elif self.model_type == "sentence-transformers":
            from sentence_transformers import SentenceTransformer
            self.model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)
            self.model_name = SENTENCE_TRANSFORMER_MODEL
            print(f"ğŸ”„ ä½¿ç”¨ Sentence Transformer: {self.model_name}")
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ embedding é¡å‹: {self.model_type}")
    
    def encode(self, texts: List[str], show_progress: bool = False) -> np.ndarray:
        """
        å°‡æ–‡æœ¬ç·¨ç¢¼ç‚ºå‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            show_progress: æ˜¯å¦é¡¯ç¤ºé€²åº¦æ¢
        
        Returns:
            åµŒå…¥å‘é‡æ•¸çµ„ (n_texts, embedding_dim)
        """
        if self.model_type == "openai":
            # OpenAI API æ‰¹é‡åµŒå…¥
            response = self.client.embeddings.create(
                model=self.model_name,
                input=texts
            )
            embeddings = [item.embedding for item in response.data]
            return np.array(embeddings)
            
        elif self.model_type == "sentence-transformers":
            # Sentence Transformer
            return self.model.encode(
                texts,
                convert_to_numpy=True,
                show_progress_bar=show_progress
            )
    
    def cosine_similarity(self, query_emb: np.ndarray, corpus_embs: np.ndarray) -> np.ndarray:
        """
        è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        
        Args:
            query_emb: æŸ¥è©¢å‘é‡ (1, embedding_dim)
            corpus_embs: èªæ–™åº«å‘é‡ (n_corpus, embedding_dim)
        
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•¸ (n_corpus,)
        """
        # ç¢ºä¿æ˜¯ 2D æ•¸çµ„
        if query_emb.ndim == 1:
            query_emb = query_emb.reshape(1, -1)
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        query_norm = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True)
        corpus_norm = corpus_embs / np.linalg.norm(corpus_embs, axis=1, keepdims=True)
        similarities = np.dot(query_norm, corpus_norm.T)[0]
        
        return similarities


# å…¨å±€åµŒå…¥æ¨¡å‹å¯¦ä¾‹
EMBED_MODEL = EmbeddingModel()


# ==================== æ•¸æ“šçµæ§‹ ====================

@dataclass
class RetrievalResult:
    """æª¢ç´¢çµæœ"""
    intent_id: str
    intent_data: Dict
    similarity_score: float
    
    def to_dict(self):
        return {
            "intent_id": self.intent_id,
            "intent_data": self.intent_data,
            "similarity_score": self.similarity_score
        }


@dataclass
class ExpandedClause:
    """æ“´å±•çš„æ¢æ–‡"""
    source_type: str  # "mother", "item", "subitem", "referenced"
    clause_id: str
    item_no: Optional[str]
    subitem_no: Optional[str]
    content: str
    raw_text: str
    similarity_score: float = 0.0
    
    def to_dict(self):
        return {
            "source_type": self.source_type,
            "clause_id": self.clause_id,
            "item_no": self.item_no,
            "subitem_no": self.subitem_no,
            "content": self.content,
            "raw_text": self.raw_text,
            "similarity_score": self.similarity_score
        }


# ==================== åµŒå…¥ç´¢å¼• ====================

class IntentIndex:
    """æ„åœ–åµŒå…¥ç´¢å¼•"""
    
    def __init__(self, intents: List[Dict]):
        """
        åˆå§‹åŒ–æ„åœ–ç´¢å¼•
        
        Args:
            intents: æ„åœ–åˆ—è¡¨
        """
        self.intents = intents
        self.intent_map = {intent["intent_id"]: intent for intent in intents}
        
        # æ§‹å»ºæª¢ç´¢èªæ–™ï¼ˆåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
        self.corpus = []
        for intent in intents:
            # çµ„åˆå¤šå€‹å­—æ®µä»¥æé«˜æª¢ç´¢è³ªé‡
            parts = [
                f"å•é¡Œ: {intent['user_query']}",
                f"å…§å®¹: {intent['excerpt']}",
            ]
            
            if intent.get("conditions"):
                parts.append(f"æ¢ä»¶: {'; '.join(intent['conditions'])}")
            
            if intent.get("category"):
                parts.append(f"é¡åˆ¥: {intent['category']}")
            
            corpus_text = " | ".join(parts)
            self.corpus.append(corpus_text)
        
        # ç”ŸæˆåµŒå…¥
        print("ğŸ”„ æ­£åœ¨ç”Ÿæˆæ„åœ–åµŒå…¥...")
        self.embeddings = EMBED_MODEL.encode(self.corpus, show_progress=True)
        print(f"âœ… å·²ç”Ÿæˆ {len(self.corpus)} å€‹æ„åœ–çš„åµŒå…¥ (ç¶­åº¦: {self.embeddings.shape[1]})")
    
    def search(self, query: str, top_k: int = 5) -> List[RetrievalResult]:
        """
        æª¢ç´¢æœ€ç›¸é—œçš„æ„åœ–
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            top_k: è¿”å›å‰ K å€‹çµæœ
        
        Returns:
            RetrievalResult åˆ—è¡¨
        """
        # æŸ¥è©¢åµŒå…¥
        query_embedding = EMBED_MODEL.encode([query], show_progress=False)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarities = EMBED_MODEL.cosine_similarity(query_embedding, self.embeddings)
        
        # ç²å– top-k ç´¢å¼•
        top_indices = similarities.argsort()[::-1][:top_k]
        
        # æ§‹å»ºçµæœ
        results = []
        for idx in top_indices:
            results.append(RetrievalResult(
                intent_id=self.intents[idx]["intent_id"],
                intent_data=self.intents[idx],
                similarity_score=float(similarities[idx])
            ))
        
        return results


# ==================== æ¢æ–‡æ“´å±• ====================

class ClauseExpander:
    """æ¢æ–‡æ“´å±•å™¨"""
    
    def __init__(self, chunks: List[Dict]):
        """
        åˆå§‹åŒ–æ¢æ–‡æ“´å±•å™¨
        
        Args:
            chunks: æ¢æ–‡ chunks åˆ—è¡¨
        """
        # å»ºç«‹å¿«é€ŸæŸ¥æ‰¾æ˜ å°„
        self.clause_map = {}  # clause_id -> chunk
        self.item_map = {}     # (clause_id, item_no) -> item
        self.subitem_map = {}  # (clause_id, item_no, subitem_no) -> subitem
        
        for chunk in chunks:
            clause = chunk["clause"]
            clause_id = clause["clause_id"]
            
            # æ¢æ–‡ç´šåˆ¥æ˜ å°„
            self.clause_map[clause_id] = chunk
            
            # é …ç›®ç´šåˆ¥æ˜ å°„
            for item in clause.get("items", []):
                item_key = (clause_id, item["item_no"])
                self.item_map[item_key] = item
                
                # æ¬¾é …ç´šåˆ¥æ˜ å°„
                for subitem in item.get("sub_items", []):
                    subitem_key = (clause_id, item["item_no"], subitem["subitem_no"])
                    self.subitem_map[subitem_key] = subitem
    
    def expand_from_intent(self, intent: Dict) -> List[ExpandedClause]:
        """
        æ ¹æ“šæ„åœ–æ“´å±•ç›¸é—œæ¢æ–‡
        
        Args:
            intent: æ„åœ–æ•¸æ“š
        
        Returns:
            ExpandedClause åˆ—è¡¨
        """
        expanded = []
        clause_id = intent["clause_id"]
        item_no = intent.get("item_no")
        subitem_no = intent.get("subitem_no")
        
        # 1. æ¯æ¢æ–‡ï¼ˆç¸½æ˜¯åŒ…å«ï¼‰
        if clause_id in self.clause_map:
            chunk = self.clause_map[clause_id]
            clause = chunk["clause"]
            
            expanded.append(ExpandedClause(
                source_type="mother",
                clause_id=clause_id,
                item_no=None,
                subitem_no=None,
                content=clause["context"],
                raw_text=clause["raw_text"]
            ))
        
        # 2. ç‰¹å®šé …ç›®ï¼ˆå¦‚æœæ„åœ–é‡å°æŸå€‹é …ç›®ï¼‰
        if item_no:
            item_key = (clause_id, item_no)
            if item_key in self.item_map:
                item = self.item_map[item_key]
                
                expanded.append(ExpandedClause(
                    source_type="item",
                    clause_id=clause_id,
                    item_no=item_no,
                    subitem_no=None,
                    content=item["context"],
                    raw_text=item["raw_text"]
                ))
        
        # 3. ç‰¹å®šæ¬¾é …ï¼ˆå¦‚æœæ„åœ–é‡å°æŸå€‹æ¬¾é …ï¼‰
        if item_no and subitem_no:
            subitem_key = (clause_id, item_no, subitem_no)
            if subitem_key in self.subitem_map:
                subitem = self.subitem_map[subitem_key]
                
                expanded.append(ExpandedClause(
                    source_type="subitem",
                    clause_id=clause_id,
                    item_no=item_no,
                    subitem_no=subitem_no,
                    content=subitem["context"],
                    raw_text=subitem["raw_text"]
                ))
        
        # 4. è¢«å¼•ç”¨çš„æ¢æ–‡
        for ref_clause_id in intent.get("referenced_clauses", []):
            if ref_clause_id in self.clause_map:
                ref_chunk = self.clause_map[ref_clause_id]
                ref_clause = ref_chunk["clause"]
                
                expanded.append(ExpandedClause(
                    source_type="referenced",
                    clause_id=ref_clause_id,
                    item_no=None,
                    subitem_no=None,
                    content=ref_clause["context"],
                    raw_text=ref_clause["raw_text"]
                ))
        
        return expanded


# ==================== é‡æ’åº ====================

class SemanticReranker:
    """èªç¾©é‡æ’åºå™¨"""
    
    @staticmethod
    def rerank(query: str, 
               clauses: List[ExpandedClause], 
               top_k: int = 3) -> List[ExpandedClause]:
        """
        ä½¿ç”¨èªç¾©ç›¸ä¼¼åº¦é‡æ’åºæ¢æ–‡
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            clauses: å€™é¸æ¢æ–‡åˆ—è¡¨
            top_k: è¿”å›å‰ K å€‹çµæœ
        
        Returns:
            é‡æ’åºå¾Œçš„ ExpandedClause åˆ—è¡¨
        """
        if not clauses:
            return []
        
        # æå–æ–‡æœ¬
        texts = [clause.raw_text for clause in clauses]
        
        # è¨ˆç®—åµŒå…¥
        query_emb = EMBED_MODEL.encode([query], show_progress=False)
        clause_embs = EMBED_MODEL.encode(texts, show_progress=False)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarities = EMBED_MODEL.cosine_similarity(query_emb, clause_embs)
        
        # æ›´æ–°ç›¸ä¼¼åº¦åˆ†æ•¸
        for i, clause in enumerate(clauses):
            clause.similarity_score = float(similarities[i])
        
        # æ’åºä¸¦è¿”å› top-k
        sorted_clauses = sorted(clauses, key=lambda x: x.similarity_score, reverse=True)
        return sorted_clauses[:top_k]


# ==================== æª¢ç´¢å¼•æ“ ====================

class RetrievalEngine:
    """RAG æª¢ç´¢å¼•æ“"""
    
    def __init__(self, intents_path: str, chunks_path: str):
        """
        åˆå§‹åŒ–æª¢ç´¢å¼•æ“
        
        Args:
            intents_path: æ„åœ– JSON æ–‡ä»¶è·¯å¾‘
            chunks_path: Chunks JSON æ–‡ä»¶è·¯å¾‘
        """
        # è¼‰å…¥æ•¸æ“š
        print("ğŸ“¥ è¼‰å…¥æ„åœ–æ•¸æ“š...")
        with open(intents_path, "r", encoding="utf-8") as f:
            intents_data = json.load(f)
        self.intents = intents_data["intents"]
        
        print("ğŸ“¥ è¼‰å…¥æ¢æ–‡æ•¸æ“š...")
        with open(chunks_path, "r", encoding="utf-8") as f:
            chunks_data = json.load(f)
        self.chunks = chunks_data["chunks"]
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.intent_index = IntentIndex(self.intents)
        self.clause_expander = ClauseExpander(self.chunks)
        self.reranker = SemanticReranker()
        
        print("âœ… æª¢ç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def retrieve(self,
                query: str,
                top_k_intents: int = 5,
                top_k_clauses: int = 3,
                include_metadata: bool = True) -> Dict:
        """
        æª¢ç´¢ç›¸é—œæ¢æ–‡
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            top_k_intents: æª¢ç´¢å‰ K å€‹æ„åœ–
            top_k_clauses: è¿”å›å‰ K å€‹æ¢æ–‡
            include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•¸æ“š
        
        Returns:
            æª¢ç´¢çµæœå­—å…¸
        """
        # Step 1: æ„åœ–æª¢ç´¢
        intent_results = self.intent_index.search(query, top_k=top_k_intents)
        
        # Step 2: æ¢æ–‡æ“´å±•
        candidate_clauses = []
        for intent_result in intent_results:
            expanded = self.clause_expander.expand_from_intent(intent_result.intent_data)
            candidate_clauses.extend(expanded)
        
        # å»é‡ï¼ˆåŸºæ–¼ clause_id + item_no + subitem_noï¼‰
        seen = set()
        unique_clauses = []
        for clause in candidate_clauses:
            key = (clause.clause_id, clause.item_no, clause.subitem_no)
            if key not in seen:
                seen.add(key)
                unique_clauses.append(clause)
        
        # Step 3: é‡æ’åº
        top_clauses = self.reranker.rerank(query, unique_clauses, top_k=top_k_clauses)
        
        # æ§‹å»ºçµæœ
        result = {
            "query": query,
            "top_intents": [r.to_dict() for r in intent_results] if include_metadata else None,
            "top_clauses": [c.to_dict() for c in top_clauses]
        }
        
        return result
    
    def get_context_for_llm(self, query: str, **kwargs) -> str:
        """
        ç²å–ç”¨æ–¼ LLM çš„æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            **kwargs: å‚³éçµ¦ retrieve çš„å…¶ä»–åƒæ•¸
        
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        result = self.retrieve(query, **kwargs)
        
        context_parts = []
        for i, clause in enumerate(result["top_clauses"], 1):
            source_label = {
                "mother": "æ¯æ¢æ–‡",
                "item": "å­é …ç›®",
                "subitem": "å­æ¬¾é …",
                "referenced": "å¼•ç”¨æ¢æ–‡"
            }.get(clause["source_type"], "å…¶ä»–")
            
            location = clause["clause_id"]
            if clause["item_no"]:
                location += f" ç¬¬{clause['item_no']}é …"
            if clause["subitem_no"]:
                location += f" ç¬¬{clause['subitem_no']}æ¬¾"
            
            context_parts.append(
                f"ã€æ¢æ–‡ {i}ã€‘{source_label} - {location}\n"
                f"å…§å®¹: {clause['content']}\n"
                f"ç›¸ä¼¼åº¦: {clause['similarity_score']:.3f}\n"
            )
        
        return "\n".join(context_parts)


# ==================== ä¸»ç¨‹åºï¼ˆæ¸¬è©¦ï¼‰ ====================

if __name__ == "__main__":
    # åˆå§‹åŒ–æª¢ç´¢å¼•æ“
    intents_path = os.path.join(INDEX_DIR, "intents_v2.json")
    chunks_path = os.path.join(INDEX_DIR, "chunks_structured_with_intents_v2.json")
    
    if not os.path.exists(intents_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ„åœ–æ–‡ä»¶: {intents_path}")
        print("è«‹å…ˆé‹è¡Œ intent_generator.py")
        exit(1)
    
    if not os.path.exists(chunks_path):
        print(f"âŒ æ‰¾ä¸åˆ° chunks æ–‡ä»¶: {chunks_path}")
        print("è«‹å…ˆé‹è¡Œ chunk_generator.py å’Œ intent_generator.py")
        exit(1)
    
    engine = RetrievalEngine(intents_path, chunks_path)
    
    # æ¸¬è©¦æŸ¥è©¢
    test_queries = [
        "ä»€éº¼æƒ…æ³ä¸‹å¯ä»¥ç”³è«‹æ—…éŠå»¶èª¤è³ å„Ÿï¼Ÿ",
        "è¡Œæéºå¤±å¾Œæ‡‰è©²å¦‚ä½•ç”³è«‹ç†è³ ï¼Ÿ",
        "å“ªäº›åŸå› å±¬æ–¼ä¸å¯ç†è³ ç¯„åœï¼Ÿ",
        "ç­æ©Ÿå»¶èª¤å¤šä¹…å¯ä»¥ç†è³ ï¼Ÿ"
    ]
    
    print("\n" + "="*60)
    print("ğŸ§ª æ¸¬è©¦æª¢ç´¢å¼•æ“")
    print("="*60)
    
    for query in test_queries:
        print(f"\nğŸ“ æŸ¥è©¢: {query}")
        print("-" * 60)
        
        context = engine.get_context_for_llm(query, top_k_intents=5, top_k_clauses=3)
        print(context)
        print("-" * 60)
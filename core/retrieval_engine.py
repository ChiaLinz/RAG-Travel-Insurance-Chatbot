"""
Retrieval Engine - æ··åˆæª¢ç´¢å¼•æ“

æ–°å¢åŠŸèƒ½ï¼š
1. BM25 + Semantic æ··åˆæª¢ç´¢
2. Cross-Encoder Reranking
3. èªç¾©æ¨™ç±¤éæ¿¾
4. å‹•æ…‹ Top-K èª¿æ•´
"""

import json
import os
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from config import INDEX_DIR, EMBEDDING_TYPE, OPENAI_EMBEDDING_MODEL, SENTENCE_TRANSFORMER_MODEL
from transformers import logging 
logging.set_verbosity_error()



# ==================== åµŒå…¥æ¨¡å‹åˆå§‹åŒ– ====================

class EmbeddingModel:
    """çµ±ä¸€çš„åµŒå…¥æ¨¡å‹æ¥å£"""
    
    def __init__(self):
        self.model_type = EMBEDDING_TYPE
        
        if self.model_type == "openai":
            from openai import OpenAI
            from dotenv import load_dotenv
            load_dotenv()
            self.client = OpenAI()
            self.model_name = OPENAI_EMBEDDING_MODEL
            print(f"ğŸ”„ ä½¿ç”¨ OpenAI Embedding: {self.model_name}")
            
        elif self.model_type == "sentence-transformers":
            from sentence_transformers import SentenceTransformer
            self.model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)
            self.model_name = SENTENCE_TRANSFORMER_MODEL
            print(f"ğŸ”„ ä½¿ç”¨ Sentence Transformer: {self.model_name}")
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ embedding é¡å‹: {self.model_type}")
    
    def encode(self, texts: List[str], show_progress: bool = False) -> np.ndarray:
        """å°‡æ–‡æœ¬ç·¨ç¢¼ç‚ºå‘é‡"""
        if self.model_type == "openai":
            response = self.client.embeddings.create(
                model=self.model_name,
                input=texts
            )
            embeddings = [item.embedding for item in response.data]
            return np.array(embeddings)
            
        elif self.model_type == "sentence-transformers":
            return self.model.encode(
                texts,
                convert_to_numpy=True,
                show_progress_bar=show_progress
            )
    
    def cosine_similarity(self, query_emb: np.ndarray, corpus_embs: np.ndarray) -> np.ndarray:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        if query_emb.ndim == 1:
            query_emb = query_emb.reshape(1, -1)
        
        query_norm = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True)
        corpus_norm = corpus_embs / np.linalg.norm(corpus_embs, axis=1, keepdims=True)
        similarities = np.dot(query_norm, corpus_norm.T)[0]
        
        return similarities


# å…¨å±€åµŒå…¥æ¨¡å‹å¯¦ä¾‹
EMBED_MODEL = EmbeddingModel()


# ==================== BM25 æª¢ç´¢å™¨ ====================

class BM25Retriever:
    """BM25 é—œéµè©æª¢ç´¢å™¨"""
    
    def __init__(self, corpus: List[str], k1: float = 1.5, b: float = 0.75):
        """
        åˆå§‹åŒ– BM25
        
        Args:
            corpus: æ–‡æœ¬èªæ–™åº«
            k1: BM25 åƒæ•¸ï¼ˆæ§åˆ¶è©é »é£½å’Œåº¦ï¼‰
            b: BM25 åƒæ•¸ï¼ˆæ§åˆ¶æ–‡æª”é•·åº¦æ­¸ä¸€åŒ–ï¼‰
        """
        try:
            from rank_bm25 import BM25Okapi
            import jieba
            
            self.jieba = jieba
            self.tokenize = lambda x: list(jieba.cut(x))
            
            # åˆ†è©
            tokenized_corpus = [self.tokenize(doc) for doc in corpus]
            
            # æ§‹å»º BM25
            self.bm25 = BM25Okapi(tokenized_corpus, k1=k1, b=b)
            self.corpus = corpus
            
            print(f"âœ… BM25 ç´¢å¼•æ§‹å»ºå®Œæˆï¼ˆ{len(corpus)} æ–‡æª”ï¼‰")
            
        except ImportError:
            print("âš ï¸  æœªå®‰è£ rank_bm25ï¼ŒBM25 æª¢ç´¢å°‡ä¸å¯ç”¨")
            print("   å®‰è£: pip install rank-bm25 jieba")
            self.bm25 = None
    
    def search(self, query: str, top_k: int = 5) -> np.ndarray:
        """
        BM25 æª¢ç´¢
        
        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            top_k: è¿”å›å‰ K å€‹çµæœ
        
        Returns:
            BM25 åˆ†æ•¸æ•¸çµ„
        """
        if self.bm25 is None:
            # å¦‚æœ BM25 ä¸å¯ç”¨ï¼Œè¿”å›é›¶åˆ†æ•¸
            return np.zeros(len(self.corpus))
        
        tokenized_query = self.tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)
        
        return scores


# ==================== Cross-Encoder Reranker ====================

class CrossEncoderReranker:
    """Cross-Encoder é‡æ’åºå™¨"""
    
    def __init__(self, model_name: str = "BAAI/bge-reranker-large"):
        """
        åˆå§‹åŒ– Cross-Encoder
        
        Args:
            model_name: æ¨¡å‹åç¨±
        """
        try:
            from sentence_transformers import CrossEncoder
            
            print(f"ğŸ”„ è¼‰å…¥ Cross-Encoder: {model_name}")
            self.model = CrossEncoder(model_name)
            self.enabled = True
            print(f"âœ… Cross-Encoder è¼‰å…¥å®Œæˆ")
            
        except ImportError:
            print("âš ï¸  æœªå®‰è£ sentence-transformersï¼ŒCross-Encoder å°‡ä¸å¯ç”¨")
            print("   å®‰è£: pip install sentence-transformers")
            self.enabled = False
        except Exception as e:
            print(f"âš ï¸  Cross-Encoder è¼‰å…¥å¤±æ•—: {e}")
            self.enabled = False
    
    def rerank(self, query: str, documents: List[str], top_k: int = 3) -> Tuple[List[int], List[float]]:
        """
        é‡æ’åºæ–‡æª”
        
        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            documents: å€™é¸æ–‡æª”åˆ—è¡¨
            top_k: è¿”å›å‰ K å€‹çµæœ
        
        Returns:
            (ç´¢å¼•åˆ—è¡¨, åˆ†æ•¸åˆ—è¡¨)
        """
        if not self.enabled or not documents:
            # å¦‚æœä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹é †åº
            return list(range(min(top_k, len(documents)))), [1.0] * min(top_k, len(documents))
        
        # æ§‹å»º query-document pairs
        pairs = [[query, doc] for doc in documents]
        
        # é æ¸¬åˆ†æ•¸
        scores = self.model.predict(pairs)
        
        # æ’åº
        ranked_indices = np.argsort(scores)[::-1][:top_k]
        ranked_scores = [scores[i] for i in ranked_indices]
        
        return ranked_indices.tolist(), ranked_scores


# ==================== æ•¸æ“šçµæ§‹ ====================

@dataclass
class RetrievalResult:
    """æª¢ç´¢çµæœ"""
    intent_id: str
    intent_data: Dict
    similarity_score: float
    bm25_score: float = 0.0
    hybrid_score: float = 0.0
    
    def to_dict(self):
        return {
            "intent_id": self.intent_id,
            "intent_data": self.intent_data,
            "similarity_score": self.similarity_score,
            "bm25_score": self.bm25_score,
            "hybrid_score": self.hybrid_score
        }


@dataclass
class ExpandedClause:
    """æ“´å±•çš„æ¢æ–‡"""
    source_type: str
    clause_id: str
    item_no: Optional[str]
    subitem_no: Optional[str]
    content: str
    raw_text: str
    similarity_score: float = 0.0
    rerank_score: float = 0.0
    
    def to_dict(self):
        return {
            "source_type": self.source_type,
            "clause_id": self.clause_id,
            "item_no": self.item_no,
            "subitem_no": self.subitem_no,
            "content": self.content,
            "raw_text": self.raw_text,
            "similarity_score": self.similarity_score,
            "rerank_score": self.rerank_score
        }


# ==================== æ„åœ–ç´¢å¼•ï¼ˆæ··åˆæª¢ç´¢ï¼‰====================

class IntentIndex:
    """æ„åœ–åµŒå…¥ç´¢å¼•ï¼ˆæ··åˆæª¢ç´¢ç‰ˆï¼‰"""
    
    def __init__(self, intents: List[Dict], use_bm25: bool = True):
        """
        åˆå§‹åŒ–æ„åœ–ç´¢å¼•
        
        Args:
            intents: æ„åœ–åˆ—è¡¨
            use_bm25: æ˜¯å¦å•Ÿç”¨ BM25
        """
        self.intents = intents
        self.intent_map = {intent["intent_id"]: intent for intent in intents}
        
        # æ§‹å»ºæª¢ç´¢èªæ–™
        self.corpus = []
        for intent in intents:
            parts = [
                f"å•é¡Œ: {intent['user_query']}",
                f"å…§å®¹: {intent['excerpt']}",
            ]
            
            if intent.get("conditions"):
                parts.append(f"æ¢ä»¶: {'; '.join(intent['conditions'])}")
            
            if intent.get("category"):
                parts.append(f"é¡åˆ¥: {intent['category']}")
            
            corpus_text = " | ".join(parts)
            self.corpus.append(corpus_text)
        
        # 1. èªç¾©åµŒå…¥
        print("ğŸ”„ æ­£åœ¨ç”Ÿæˆæ„åœ–åµŒå…¥ï¼ˆSemanticï¼‰...")
        self.embeddings = EMBED_MODEL.encode(self.corpus, show_progress=True)
        print(f"âœ… å·²ç”Ÿæˆ {len(self.corpus)} å€‹æ„åœ–çš„åµŒå…¥ (ç¶­åº¦: {self.embeddings.shape[1]})")
        
        # 2. BM25 ç´¢å¼•
        self.bm25_retriever = None
        if use_bm25:
            print("ğŸ”„ æ§‹å»º BM25 ç´¢å¼•...")
            self.bm25_retriever = BM25Retriever(self.corpus)
    
    def search(self, 
               query: str, 
               top_k: int = 5,
               semantic_weight: float = 0.85,
               bm25_weight: float = 0.15,
               semantic_tags_filter: Optional[List[str]] = None) -> List[RetrievalResult]:
        """
        æ··åˆæª¢ç´¢ï¼ˆå¢å¼·ç‰ˆï¼‰
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            top_k: è¿”å›å‰ K å€‹çµæœ
            semantic_weight: èªç¾©æª¢ç´¢æ¬Šé‡
            bm25_weight: BM25 æ¬Šé‡
            semantic_tags_filter: èªç¾©æ¨™ç±¤éæ¿¾ï¼ˆå¯é¸ï¼‰
        
        Returns:
            RetrievalResult åˆ—è¡¨
        """
        # 1. èªç¾©æª¢ç´¢
        query_embedding = EMBED_MODEL.encode([query], show_progress=False)
        semantic_scores = EMBED_MODEL.cosine_similarity(query_embedding, self.embeddings)
        
        # 2. BM25 æª¢ç´¢
        if self.bm25_retriever is not None:
            bm25_scores = self.bm25_retriever.search(query, top_k=top_k)
            # æ­¸ä¸€åŒ– BM25 åˆ†æ•¸
            if bm25_scores.max() > 0:
                bm25_scores = bm25_scores / bm25_scores.max()
        else:
            bm25_scores = np.zeros_like(semantic_scores)
        
        # 3. æ··åˆåˆ†æ•¸
        hybrid_scores = semantic_weight * semantic_scores + bm25_weight * bm25_scores
        
        # 4. èªç¾©æ¨™ç±¤éæ¿¾
        if semantic_tags_filter:
            for i, intent in enumerate(self.intents):
                intent_tags = intent.get("semantic_tags", [])
                # å¦‚æœæ²’æœ‰åŒ¹é…çš„æ¨™ç±¤ï¼Œé™ä½åˆ†æ•¸
                if not any(tag in intent_tags for tag in semantic_tags_filter):
                    hybrid_scores[i] *= 0.5  # é™ä½ 50%
        
        # 5. ç²å– top-k
        top_indices = hybrid_scores.argsort()[::-1][:top_k]
        
        # 6. æ§‹å»ºçµæœ
        results = []
        for idx in top_indices:
            results.append(RetrievalResult(
                intent_id=self.intents[idx]["intent_id"],
                intent_data=self.intents[idx],
                similarity_score=float(semantic_scores[idx]),
                bm25_score=float(bm25_scores[idx]),
                hybrid_score=float(hybrid_scores[idx])
            ))
        
        return results


# ==================== æ¢æ–‡æ“´å±•å™¨ ====================

class ClauseExpander:
    """æ¢æ–‡æ“´å±•å™¨"""
    
    def __init__(self, chunks: List[Dict]):
        """åˆå§‹åŒ–æ¢æ–‡æ“´å±•å™¨"""
        self.clause_map = {}
        self.item_map = {}
        self.subitem_map = {}
        
        for chunk in chunks:
            clause = chunk["clause"]
            clause_id = clause["clause_id"]
            
            self.clause_map[clause_id] = chunk
            
            for item in clause.get("items", []):
                item_key = (clause_id, item["item_no"])
                self.item_map[item_key] = item
                
                for subitem in item.get("sub_items", []):
                    subitem_key = (clause_id, item["item_no"], subitem["subitem_no"])
                    self.subitem_map[subitem_key] = subitem
    
    def expand_from_intent(self, intent: Dict) -> List[ExpandedClause]:
        """æ ¹æ“šæ„åœ–æ“´å±•ç›¸é—œæ¢æ–‡"""
        expanded = []
        clause_id = intent["clause_id"]
        item_no = intent.get("item_no")
        subitem_no = intent.get("subitem_no")
        
        # æ¯æ¢æ–‡
        if clause_id in self.clause_map:
            chunk = self.clause_map[clause_id]
            clause = chunk["clause"]
            
            expanded.append(ExpandedClause(
                source_type="mother",
                clause_id=clause_id,
                item_no=None,
                subitem_no=None,
                content=clause["context"],
                raw_text=clause["raw_text"]
            ))
        
        # ç‰¹å®šé …ç›®
        if item_no:
            item_key = (clause_id, item_no)
            if item_key in self.item_map:
                item = self.item_map[item_key]
                
                expanded.append(ExpandedClause(
                    source_type="item",
                    clause_id=clause_id,
                    item_no=item_no,
                    subitem_no=None,
                    content=item["context"],
                    raw_text=item["raw_text"]
                ))
        
        # ç‰¹å®šæ¬¾é …
        if item_no and subitem_no:
            subitem_key = (clause_id, item_no, subitem_no)
            if subitem_key in self.subitem_map:
                subitem = self.subitem_map[subitem_key]
                
                expanded.append(ExpandedClause(
                    source_type="subitem",
                    clause_id=clause_id,
                    item_no=item_no,
                    subitem_no=subitem_no,
                    content=subitem["context"],
                    raw_text=subitem["raw_text"]
                ))
        
        # è¢«å¼•ç”¨çš„æ¢æ–‡
        for ref_clause_id in intent.get("referenced_clauses", []):
            if ref_clause_id in self.clause_map:
                ref_chunk = self.clause_map[ref_clause_id]
                ref_clause = ref_chunk["clause"]
                
                expanded.append(ExpandedClause(
                    source_type="referenced",
                    clause_id=ref_clause_id,
                    item_no=None,
                    subitem_no=None,
                    content=ref_clause["context"],
                    raw_text=ref_clause["raw_text"]
                ))
        
        return expanded


# ==================== æª¢ç´¢å¼•æ“ï¼ˆæ··åˆç‰ˆï¼‰====================

class RetrievalEngine:
    """RAG æª¢ç´¢å¼•æ“ï¼ˆæ··åˆæª¢ç´¢ + Cross-Encoderï¼‰"""
    
    def __init__(self, 
                 intents_path: str, 
                 chunks_path: str,
                 use_bm25: bool = True,
                 use_cross_encoder: bool = True):
        """
        åˆå§‹åŒ–æª¢ç´¢å¼•æ“
        
        Args:
            intents_path: æ„åœ– JSON æ–‡ä»¶è·¯å¾‘
            chunks_path: Chunks JSON æ–‡ä»¶è·¯å¾‘
            use_bm25: æ˜¯å¦å•Ÿç”¨ BM25
            use_cross_encoder: æ˜¯å¦å•Ÿç”¨ Cross-Encoder
        """
        # è¼‰å…¥æ•¸æ“š
        print("ğŸ“¥ è¼‰å…¥æ„åœ–æ•¸æ“š...")
        with open(intents_path, "r", encoding="utf-8") as f:
            intents_data = json.load(f)
        self.intents = intents_data["intents"]
        
        print("ğŸ“¥ è¼‰å…¥æ¢æ–‡æ•¸æ“š...")
        with open(chunks_path, "r", encoding="utf-8") as f:
            chunks_data = json.load(f)
        self.chunks = chunks_data["chunks"]
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.intent_index = IntentIndex(self.intents, use_bm25=use_bm25)
        self.clause_expander = ClauseExpander(self.chunks)
        
        # Cross-Encoder
        self.use_cross_encoder = use_cross_encoder
        if use_cross_encoder:
            self.cross_encoder = CrossEncoderReranker()
        else:
            self.cross_encoder = None
        
        print("âœ… æª¢ç´¢å¼•æ“ åˆå§‹åŒ–å®Œæˆ")
    
    def _detect_semantic_tags(self, query: str) -> List[str]:
        """æª¢æ¸¬æŸ¥è©¢ä¸­çš„èªç¾©æ¨™ç±¤"""
        tags = []
        
        # å‹•ä½œè©æ˜ å°„
        action_map = {
            "éºå¤±": ["éºå¤±", "ä¸Ÿ", "å¼„ä¸Ÿ", "ä¸è¦‹", "å¤±ç«Š"],
            "å»¶èª¤": ["å»¶èª¤", "æ™šé»", "delay", "èª¤é»"],
            "æå¤±": ["æå¤±", "æå£", "æ¯€æ"],
            "å–æ¶ˆ": ["å–æ¶ˆ", "ä¸­æ­¢"],
            "æ›´æ”¹": ["æ›´æ”¹", "è®Šæ›´", "æ”¹è®Š"],
            "ç«Šç›œ": ["å·", "ç«Š", "æ¶"],
        }
        
        for tag, keywords in action_map.items():
            if any(kw in query for kw in keywords):
                tags.append(tag)
        
        # ç‰¹æ®Šæ¨™ç±¤
        if any(kw in query for kw in ["ä¸", "å“ªäº›", "é™¤å¤–"]):
            tags.append("ä¸ä¿")
        
        return tags
    
    def _smart_top_k(self, query: str) -> Dict[str, int]:
        """å‹•æ…‹èª¿æ•´ top_k"""
        # è¤‡æ•¸å•é¡Œ â†’ å¢åŠ å¬å›
        if any(kw in query for kw in ["å“ªäº›", "æ‰€æœ‰", "å…¨éƒ¨", "ä»€éº¼"]):
            return {"intents": 10, "clauses": 5}
        
        # ç°¡å–®å•é¡Œ â†’ æ¸›å°‘å¬å›
        elif any(kw in query for kw in ["å¤šä¹…", "å¹¾å°æ™‚", "å¹¾å¤©"]):
            return {"intents": 3, "clauses": 2}
        
        # é»˜èª
        else:
            return {"intents": 5, "clauses": 3}
    
    def retrieve(self,
                query: str,
                top_k_intents: Optional[int] = None,
                top_k_clauses: Optional[int] = None,
                include_metadata: bool = True,
                auto_adjust_topk: bool = True) -> Dict:
        """
        æª¢ç´¢ç›¸é—œæ¢æ–‡ï¼ˆå¢å¼·ç‰ˆï¼‰
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            top_k_intents: æª¢ç´¢å‰ K å€‹æ„åœ–ï¼ˆNone = è‡ªå‹•ï¼‰
            top_k_clauses: è¿”å›å‰ K å€‹æ¢æ–‡ï¼ˆNone = è‡ªå‹•ï¼‰
            include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•¸æ“š
            auto_adjust_topk: æ˜¯å¦è‡ªå‹•èª¿æ•´ top_k
        
        Returns:
            æª¢ç´¢çµæœå­—å…¸
        """
        # å‹•æ…‹èª¿æ•´ top_k
        if auto_adjust_topk:
            smart_k = self._smart_top_k(query)
            top_k_intents = top_k_intents or smart_k["intents"]
            top_k_clauses = top_k_clauses or smart_k["clauses"]
        else:
            top_k_intents = top_k_intents or 5
            top_k_clauses = top_k_clauses or 3
        
        # æª¢æ¸¬èªç¾©æ¨™ç±¤
        semantic_tags = self._detect_semantic_tags(query)
        
        # Stage 1: æ„åœ–æª¢ç´¢ï¼ˆæ··åˆæª¢ç´¢ï¼‰
        intent_results = self.intent_index.search(
            query, 
            top_k=top_k_intents,
            semantic_tags_filter=semantic_tags if semantic_tags else None
        )
        
        # Stage 2: æ¢æ–‡æ“´å±•
        candidate_clauses = []
        for intent_result in intent_results:
            expanded = self.clause_expander.expand_from_intent(intent_result.intent_data)
            candidate_clauses.extend(expanded)
        
        # å»é‡
        seen = set()
        unique_clauses = []
        for clause in candidate_clauses:
            key = (clause.clause_id, clause.item_no, clause.subitem_no)
            if key not in seen:
                seen.add(key)
                unique_clauses.append(clause)
        
        # Stage 3: Cross-Encoder é‡æ’åº
        if self.use_cross_encoder and self.cross_encoder and self.cross_encoder.enabled:
            # æå–æ–‡æœ¬
            texts = [clause.raw_text for clause in unique_clauses]
            
            # é‡æ’åº
            ranked_indices, rerank_scores = self.cross_encoder.rerank(
                query, texts, top_k=top_k_clauses
            )
            
            # æ›´æ–°åˆ†æ•¸ä¸¦é¸æ“‡ top-k
            top_clauses = []
            for idx, score in zip(ranked_indices, rerank_scores):
                clause = unique_clauses[idx]
                clause.rerank_score = score
                top_clauses.append(clause)
        else:
            # èªç¾©é‡æ’åº
            query_emb = EMBED_MODEL.encode([query], show_progress=False)
            clause_texts = [clause.raw_text for clause in unique_clauses]
            clause_embs = EMBED_MODEL.encode(clause_texts, show_progress=False)
            
            similarities = EMBED_MODEL.cosine_similarity(query_emb, clause_embs)
            
            for i, clause in enumerate(unique_clauses):
                clause.similarity_score = float(similarities[i])
                clause.rerank_score = float(similarities[i])
            
            sorted_clauses = sorted(unique_clauses, key=lambda x: x.similarity_score, reverse=True)
            top_clauses = sorted_clauses[:top_k_clauses]
        
        # æ§‹å»ºçµæœ
        result = {
            "query": query,
            "detected_semantic_tags": semantic_tags,
            "top_k_intents": top_k_intents,
            "top_k_clauses": top_k_clauses,
            "top_intents": [r.to_dict() for r in intent_results] if include_metadata else None,
            "top_clauses": [c.to_dict() for c in top_clauses]
        }
        
        return result
    
    def get_context_for_llm(self, query: str, **kwargs) -> str:
        """ç²å–ç”¨æ–¼ LLM çš„æ ¼å¼åŒ–ä¸Šä¸‹æ–‡"""
        result = self.retrieve(query, **kwargs)
        
        context_parts = []
        for i, clause in enumerate(result["top_clauses"], 1):
            source_label = {
                "mother": "æ¯æ¢æ–‡",
                "item": "å­é …ç›®",
                "subitem": "å­æ¬¾é …",
                "referenced": "å¼•ç”¨æ¢æ–‡"
            }.get(clause["source_type"], "å…¶ä»–")
            
            location = clause["clause_id"]
            if clause["item_no"]:
                location += f" ç¬¬{clause['item_no']}é …"
            if clause["subitem_no"]:
                location += f" ç¬¬{clause['subitem_no']}æ¬¾"
            
            # é¡¯ç¤º rerank åˆ†æ•¸
            score = clause.get("rerank_score", clause.get("similarity_score", 0))
            
            context_parts.append(
                f"ã€æ¢æ–‡ {i}ã€‘{source_label} - {location}\n"
                f"å…§å®¹: {clause['content']}\n"
                f"ç›¸é—œåº¦: {score:.3f}\n"
            )
        
        return "\n".join(context_parts)


# ==================== ä¸»ç¨‹åºï¼ˆæ¸¬è©¦ï¼‰====================

if __name__ == "__main__":
    # åˆå§‹åŒ–æª¢ç´¢å¼•æ“
    intents_path = os.path.join(INDEX_DIR, "intents.json")
    chunks_path = os.path.join(INDEX_DIR, "chunks_structured_with_intents.json")
    
    if not os.path.exists(intents_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ„åœ–æ–‡ä»¶: {intents_path}")
        print("è«‹å…ˆé‹è¡Œ intent_generator.py")
        exit(1)
    
    if not os.path.exists(chunks_path):
        print(f"âŒ æ‰¾ä¸åˆ°chunks æ–‡ä»¶: {chunks_path}")
        print("è«‹å…ˆé‹è¡Œ intent_generator.py")
        exit(1)
    
    engine = RetrievalEngine(
        intents_path, 
        chunks_path,
        use_bm25=True,
        use_cross_encoder=True
    )
    
    # æ¸¬è©¦æŸ¥è©¢
    test_queries = [
        "ä»€éº¼æƒ…æ³ä¸‹å¯ä»¥ç”³è«‹æ—…éŠå»¶èª¤è³ å„Ÿï¼Ÿ",
        "è¡Œæéºå¤±å¾Œæ‡‰è©²å¦‚ä½•ç”³è«‹ç†è³ ï¼Ÿ",
        "å“ªäº›åŸå› å±¬æ–¼ä¸å¯ç†è³ ç¯„åœï¼Ÿ",
        "ç­æ©Ÿå»¶èª¤å¤šä¹…å¯ä»¥ç†è³ ï¼Ÿ"
    ]
    
    print("\n" + "="*60)
    print("ğŸ§ª æ¸¬è©¦æª¢ç´¢å¼•æ“ï¼ˆæ··åˆæª¢ç´¢ + Cross-Encoderï¼‰")
    print("="*60)
    
    for query in test_queries:
        print(f"\nğŸ“ æŸ¥è©¢: {query}")
        print("-" * 60)
        
        result = engine.retrieve(query, include_metadata=True)
        
        print(f"æª¢æ¸¬åˆ°çš„èªç¾©æ¨™ç±¤: {result['detected_semantic_tags']}")
        print(f"Top-K é…ç½®: intents={result['top_k_intents']}, clauses={result['top_k_clauses']}")
        print()
        
        # é¡¯ç¤º top intents
        print("Top Intents:")
        for i, intent in enumerate(result["top_intents"][:3], 1):
            print(f"  {i}. {intent['intent_data']['user_query']}")
            print(f"     Semantic: {intent['similarity_score']:.3f} | "
                  f"BM25: {intent['bm25_score']:.3f} | "
                  f"Hybrid: {intent['hybrid_score']:.3f}")
        
        print()
        print("Top Clauses:")
        for i, clause in enumerate(result["top_clauses"], 1):
            print(f"  {i}. {clause['clause_id']}")
            print(f"     Rerank Score: {clause['rerank_score']:.3f}")
        
        print("-" * 60)

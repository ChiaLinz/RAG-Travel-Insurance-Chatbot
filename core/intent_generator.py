"""
Intent Generator - å„ªåŒ–æç¤ºè©ç‰ˆæœ¬

ä¸»è¦æ”¹é€²ï¼š
1. Few-shot Learningï¼ˆæ·»åŠ æ­£åä¾‹ï¼‰
2. æ›´åš´æ ¼çš„èªç¾©æ¨™ç±¤è¦æ±‚
3. å¾Œè™•ç†é©—è­‰èˆ‡è‡ªå‹•ä¿®æ­£
4. è³ªé‡è©•åˆ†æ©Ÿåˆ¶
"""

import json
import os
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
from openai import OpenAI
from dotenv import load_dotenv
from config import INDEX_DIR
import time
import re


# ==================== åˆå§‹åŒ– ====================

load_dotenv()
client = OpenAI()


# ==================== æ•¸æ“šçµæ§‹ ====================

@dataclass
class Intent:
    """æ„åœ–çµæ§‹"""
    intent_id: str
    clause_id: str
    item_no: Optional[str]
    subitem_no: Optional[str]
    
    # æ ¸å¿ƒå…§å®¹
    user_query: str
    excerpt: str
    
    # è©³ç´°ä¿¡æ¯
    conditions: List[str]
    exceptions: List[str]
    referenced_clauses: List[str]
    
    # åˆ†é¡
    category: str
    
    # å­—æ®µ
    query_type: str = "ç›´æ¥æŸ¥è©¢"
    semantic_tags: List[str] = None
    difficulty: str = "ç°¡å–®"
    quality_score: float = 1.0  # æ–°å¢ï¼šè³ªé‡è©•åˆ†
    
    def __post_init__(self):
        if self.semantic_tags is None:
            self.semantic_tags = []
    
    def to_dict(self):
        return asdict(self)


# ==================== Few-shot ç¯„ä¾‹ ====================

FEW_SHOT_EXAMPLES = """
ã€ç¯„ä¾‹ 1 - æ­£ç¢ºã€‘

æ¢æ–‡ï¼šç¬¬ä¸‰åä¹æ¢ è¡Œææå¤±ä¿éšªæ‰¿ä¿ç¯„åœ
å‹•ä½œé—œéµè©ï¼š["éºå¤±", "ç«Šç›œ", "æå¤±"]
å…§å®¹ï¼šè¢«ä¿éšªäººæ–¼æµ·å¤–æ—…è¡ŒæœŸé–“å…§ï¼Œå› ä¸‹åˆ—äº‹æ•…è‡´å…¶æ‰€æ“æœ‰ä¸”ç½®æ–¼è¡Œæç®±ã€æ‰‹æç®±æˆ–é¡ä¼¼å®¹å™¨å…§ä¹‹å€‹äººç‰©å“é­å—æ¯€ææˆ–æ»…å¤±...
ä¸€ã€ç«Šç›œã€å¼·ç›œèˆ‡æ¶å¥ªã€‚
äºŒã€äº¤ç”±æ‰€æ­ä¹˜ä¹‹å…¬å…±äº¤é€šå·¥å…·æ¥­è€…è¨—é‹ä¸”é ˜æœ‰è¨—é‹è¡Œæé ˜å–å–®ä¹‹éš¨è¡Œè¨—é‹è¡Œæï¼Œå› è©²å…¬å…±äº¤é€šå·¥å…·æ¥­è€…è™•ç†å¤±ç•¶æ‰€è‡´ä¹‹æ¯€æã€æ»…å¤±æˆ–éºå¤±ã€‚

âœ… æ­£ç¢ºçš„æ„åœ–ç”Ÿæˆï¼š
{
  "user_query": "è¡Œæè¢«å·äº†å¯ä»¥ç†è³ å—ï¼Ÿ",
  "excerpt": "å› ç«Šç›œã€å¼·ç›œèˆ‡æ¶å¥ªå°è‡´è¡Œæéºå¤±ï¼Œæœ¬å…¬å¸ä¾ä¿éšªå¥‘ç´„çµ¦ä»˜ä¿éšªé‡‘",
  "conditions": ["ç«Šç›œã€å¼·ç›œæˆ–æ¶å¥ª", "æµ·å¤–æ—…è¡ŒæœŸé–“", "ç½®æ–¼è¡Œæç®±å…§"],
  "exceptions": [],
  "referenced_clauses": ["ç¬¬å››åæ¢", "ç¬¬å››åä¸‰æ¢"],
  "category": "è³ å„Ÿç¯„åœ",
  "query_type": "ç›´æ¥æŸ¥è©¢",
  "semantic_tags": ["éºå¤±", "ç«Šç›œ"],  â† é—œéµï¼å¿…é ˆåŒ…å«å‹•ä½œé—œéµè©
  "difficulty": "ç°¡å–®"
}

{
  "user_query": "è¡Œæè¢«èˆªç©ºå…¬å¸å¼„ä¸Ÿäº†æ€éº¼è¾¦ï¼Ÿ",
  "excerpt": "è¨—é‹è¡Œæå› æ¥­è€…è™•ç†å¤±ç•¶å°è‡´éºå¤±å¯ç†è³ ",
  "conditions": ["è¨—é‹è¡Œæ", "é ˜æœ‰è¨—é‹å–®", "æ¥­è€…è™•ç†å¤±ç•¶"],
  "exceptions": [],
  "referenced_clauses": ["ç¬¬å››åä¸‰æ¢"],
  "category": "è³ å„Ÿç¯„åœ",
  "query_type": "ç›´æ¥æŸ¥è©¢",
  "semantic_tags": ["éºå¤±", "è¨—é‹"],  â† å¿…é ˆæ˜ç¢ºæ¨™è¨»
  "difficulty": "ç°¡å–®"
}

âŒ éŒ¯èª¤çš„æ„åœ–ç”Ÿæˆï¼š
{
  "user_query": "è¡Œæå•é¡Œæ€éº¼ç†è³ ï¼Ÿ",  â† å¤ªå¯¬æ³›
  "semantic_tags": ["ç†è³ "],  â† ç¼ºå°‘ã€Œéºå¤±ã€æ¨™ç±¤
  ...
}

---

ã€ç¯„ä¾‹ 2 - æ­£ç¢ºã€‘

æ¢æ–‡ï¼šç¬¬ä¸‰åå…­æ¢ è¡Œæå»¶èª¤ä¿éšªæ‰¿ä¿ç¯„åœ
å‹•ä½œé—œéµè©ï¼š["å»¶èª¤"]
å…§å®¹ï¼šè¢«ä¿éšªäººæ–¼æµ·å¤–æ—…è¡ŒæœŸé–“å…§ï¼Œå…¶éš¨è¡Œè¨—é‹ä¸¦å–å¾—è¨—é‹è¡Œæé ˜å–å–®ä¹‹å€‹äººè¡Œæå› å…¬å…±äº¤é€šå·¥å…·æ¥­è€…ä¹‹è™•ç†å¤±ç•¶ï¼Œè‡´å…¶åœ¨æŠµé”ç›®çš„åœ°å…­å°æ™‚å¾Œä»æœªé ˜å¾—æ™‚...

âœ… æ­£ç¢ºçš„æ„åœ–ç”Ÿæˆï¼š
{
  "user_query": "è¡Œæå»¶èª¤å¤šä¹…å¯ä»¥ç†è³ ï¼Ÿ",
  "excerpt": "æŠµé”ç›®çš„åœ°å…­å°æ™‚å¾Œä»æœªé ˜å¾—è¡Œæå¯ç†è³ ",
  "conditions": ["éš¨è¡Œè¨—é‹", "æœ‰è¨—é‹å–®", "å…­å°æ™‚å¾Œä»æœªé ˜å¾—"],
  "exceptions": [],
  "referenced_clauses": ["ç¬¬ä¸‰åä¸ƒæ¢", "ç¬¬ä¸‰åå…«æ¢"],
  "category": "è³ å„Ÿç¯„åœ",
  "query_type": "ç›´æ¥æŸ¥è©¢",
  "semantic_tags": ["å»¶èª¤"],  â† å¿…é ˆæ˜¯ã€Œå»¶èª¤ã€è€Œéã€Œéºå¤±ã€
  "difficulty": "ç°¡å–®"
}

âŒ éŒ¯èª¤çš„æ„åœ–ç”Ÿæˆï¼š
{
  "user_query": "è¡Œæå»¶èª¤å¤šä¹…å¯ä»¥ç†è³ ï¼Ÿ",
  "semantic_tags": ["éºå¤±"],  â† éŒ¯èª¤ï¼æ‡‰è©²æ˜¯ã€Œå»¶èª¤ã€
  ...
}

---

ã€ç¯„ä¾‹ 3 - å°æ¯”æ„åœ–ã€‘

âœ… æ­£ç¢ºçš„å°æ¯”æ„åœ–ï¼š
{
  "user_query": "è¡Œæå»¶èª¤å’Œè¡Œæéºå¤±æœ‰ä»€éº¼å·®åˆ¥ï¼Ÿ",
  "excerpt": "å»¶èª¤æ˜¯æŒ‡æœªèƒ½åŠæ™‚é ˜å–ï¼›éºå¤±æ˜¯æŒ‡æ¯€æã€æ»…å¤±",
  "conditions": [],
  "exceptions": [],
  "referenced_clauses": ["ç¬¬ä¸‰åå…­æ¢", "ç¬¬ä¸‰åä¹æ¢"],
  "category": "å®šç¾©èªªæ˜",
  "query_type": "å°æ¯”æŸ¥è©¢",
  "semantic_tags": ["å»¶èª¤", "éºå¤±", "å°æ¯”"],  â† å¿…é ˆåŒ…å«å…©å€‹æ¦‚å¿µ
  "difficulty": "ä¸­ç­‰"
}

---

ã€ç¯„ä¾‹ 4 - è² å‘æ„åœ–ã€‘

æ¢æ–‡ï¼šç¬¬å››åæ¢ è¡Œææå¤±ä¿éšªç‰¹åˆ¥ä¸ä¿äº‹é …ï¼ˆç‰©å“ï¼‰
æ¢æ–‡åŠŸèƒ½ï¼šä¸ä¿äº‹é …

âœ… æ­£ç¢ºçš„è² å‘æ„åœ–ï¼š
{
  "user_query": "å“ªäº›æ±è¥¿ä¸Ÿäº†ä¸èƒ½ç†è³ ï¼Ÿ",
  "excerpt": "å•†æ¥­ç”¨å“ã€è²¨å¹£ã€è­‰åˆ¸ã€ç å¯¶ã€æ‰‹æ©Ÿç­‰ä¸åœ¨ç†è³ ç¯„åœ",
  "conditions": [],
  "exceptions": ["å•†æ¥­ç”¨å“", "è²¨å¹£", "è­‰åˆ¸"],
  "referenced_clauses": [],
  "category": "é™¤å¤–è²¬ä»»",
  "query_type": "è² å‘æŸ¥è©¢",
  "semantic_tags": ["éºå¤±", "ä¸ä¿"],  â† å¿…é ˆåŒ…å«ã€Œä¸ä¿ã€
  "difficulty": "ç°¡å–®"
}

âŒ éŒ¯èª¤çš„è² å‘æ„åœ–ï¼š
{
  "user_query": "å“ªäº›æ±è¥¿ä¸Ÿäº†ä¸èƒ½ç†è³ ï¼Ÿ",
  "semantic_tags": ["éºå¤±"],  â† ç¼ºå°‘ã€Œä¸ä¿ã€æ¨™ç±¤
  ...
}
"""


# ==================== å„ªåŒ–æç¤ºè© ====================

INTENT_GENERATION_PROMPT_= """ä½ æ˜¯ä¿éšªæ¢æ¬¾åˆ†æå°ˆå®¶ã€‚è«‹ä»”ç´°åˆ†æä»¥ä¸‹æ¢æ–‡ï¼Œç”Ÿæˆ 5-8 å€‹é«˜è³ªé‡çš„ç”¨æˆ¶å•é¡Œï¼ˆæ„åœ–ï¼‰ã€‚

æ¢æ–‡ä¿¡æ¯ï¼š
æ¢æ–‡ç·¨è™Ÿï¼š{clause_no}
æ¢æ–‡æ¨™é¡Œï¼š{clause_title}
æ¢æ–‡åŠŸèƒ½ï¼š{clause_function}
ä¿éšªé¡å‹ï¼š{insurance_types}
å‹•ä½œé—œéµè©ï¼š{action_keywords}
æ¢æ–‡å…§å®¹ï¼š
{context}

**é‡è¦è¦å‰‡ï¼ˆå¿…é ˆéµå®ˆï¼‰**ï¼š

1. **èªç¾©æ¨™ç±¤ (semantic_tags) è¦å‰‡**ï¼š
   - å¿…é ˆåŒ…å«æ¢æ–‡çš„ã€Œå‹•ä½œé—œéµè©ã€ï¼š{action_keywords}
   - å¦‚æœæ¢æ–‡åŠŸèƒ½æ˜¯ã€Œä¸ä¿äº‹é …ã€ï¼Œå¿…é ˆåŒ…å« "ä¸ä¿"
   - å¦‚æœæ˜¯å°æ¯”æŸ¥è©¢ï¼Œå¿…é ˆåŒ…å«å…©å€‹å°æ¯”çš„æ¦‚å¿µ
   
   ç¯„ä¾‹ï¼š
   âœ… æ­£ç¢ºï¼š"è¡Œæè¢«å·" â†’ semantic_tags: ["éºå¤±", "ç«Šç›œ"]
   âŒ éŒ¯èª¤ï¼š"è¡Œæè¢«å·" â†’ semantic_tags: ["ç†è³ "]

2. **å•æ³•å¤šæ¨£åŒ–**ï¼š
   - ç›´æ¥æŸ¥è©¢ï¼šæœ€å¸¸è¦‹çš„å•æ³•ï¼ˆå¦‚ï¼š"XXå¯ä»¥ç†è³ å—ï¼Ÿ"ï¼‰
   - å£èªåŒ–ï¼šç”¨æˆ¶å¯¦éš›æœƒæ€éº¼å•ï¼ˆå¦‚ï¼š"è¡Œæå¼„ä¸Ÿäº†æ€éº¼è¾¦ï¼Ÿ"ï¼‰
   - è² å‘æŸ¥è©¢ï¼šä»€éº¼ä¸èƒ½/ä¸æœƒï¼ˆå¦‚ï¼š"å“ªäº›æƒ…æ³ä¸ç†è³ ï¼Ÿ"ï¼‰
   
3. **ç‰¹æ®Šè¦æ±‚**ï¼š
   - å¦‚æœæ¢æ–‡åŠŸèƒ½ = "ä¸ä¿äº‹é …"ï¼Œè‡³å°‘ç”Ÿæˆ 2 å€‹è² å‘æ„åœ–
   - é¿å…å¤ªå¯¬æ³›çš„å•æ³•ï¼ˆå¦‚ï¼š"ä¿éšªæ€éº¼è³ ï¼Ÿ"ï¼‰
   - æ¯å€‹ user_query å¿…é ˆæ˜ç¢ºã€å…·é«”

4. **åš´æ ¼å€åˆ†ç›¸ä¼¼æ¦‚å¿µ**ï¼š
   - "å»¶èª¤" â‰  "éºå¤±" â‰  "æå¤±" â‰  "å–æ¶ˆ" â‰  "æ›´æ”¹"
   - æ¯å€‹è©çš„èªç¾©æ¨™ç±¤éƒ½ä¸åŒï¼Œä¸èƒ½æ··æ·†

{few_shot_examples}

è«‹ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
  "intents": [
    {{
      "user_query": "å…·é«”ã€æ˜ç¢ºçš„å•é¡Œ",
      "excerpt": "é—œéµæ¢æ–‡æ‘˜éŒ„ï¼ˆ<100å­—ï¼‰",
      "conditions": ["æ¢ä»¶1", "æ¢ä»¶2"],
      "exceptions": ["ä¾‹å¤–1"],
      "referenced_clauses": ["ç¬¬XXæ¢"],
      "category": "è³ å„Ÿç¯„åœ/ç†è³ æ¢ä»¶/é™¤å¤–è²¬ä»»/ç”³è«‹æµç¨‹/å®šç¾©èªªæ˜",
      "query_type": "ç›´æ¥æŸ¥è©¢/å°æ¯”æŸ¥è©¢/è² å‘æŸ¥è©¢/æ¢ä»¶æŸ¥è©¢",
      "semantic_tags": ["å¿…é ˆåŒ…å«å‹•ä½œé—œéµè©"],
      "difficulty": "ç°¡å–®/ä¸­ç­‰/è¤‡é›œ"
    }}
  ]
}}

**æª¢æŸ¥æ¸…å–®**ï¼ˆç”Ÿæˆå‰è«‹è‡ªæˆ‘æª¢æŸ¥ï¼‰ï¼š
â–¡ semantic_tags æ˜¯å¦åŒ…å«å‹•ä½œé—œéµè©ï¼Ÿ
â–¡ å¦‚æœæ˜¯ä¸ä¿äº‹é …ï¼Œæ˜¯å¦åŒ…å«ã€Œä¸ä¿ã€æ¨™ç±¤ï¼Ÿ
â–¡ user_query æ˜¯å¦å¤ å…·é«”ï¼Ÿ
â–¡ æ˜¯å¦é¿å…äº†èˆ‡å…¶ä»–æ¦‚å¿µæ··æ·†ï¼Ÿ

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–èªªæ˜ï¼š"""


ITEM_INTENT_GENERATION_PROMPT_= """ä½ æ˜¯ä¿éšªæ¢æ¬¾åˆ†æå°ˆå®¶ã€‚è«‹åˆ†æä»¥ä¸‹é …ç›®å…§å®¹ï¼Œç”Ÿæˆ 2-3 å€‹ç²¾ç¢ºçš„ç”¨æˆ¶å•é¡Œã€‚

æ¯æ¢æ–‡ï¼š{clause_no} {clause_title}
æ¢æ–‡åŠŸèƒ½ï¼š{clause_function}
é …ç›®ç·¨è™Ÿï¼š{item_no}
å‹•ä½œé—œéµè©ï¼š{action_keywords}
é …ç›®å…§å®¹ï¼š
{item_context}

**å¿…é ˆéµå®ˆçš„è¦å‰‡**ï¼š
1. semantic_tags å¿…é ˆåŒ…å«ï¼š{action_keywords}
2. å¦‚æœæ¢æ–‡åŠŸèƒ½ = "ä¸ä¿äº‹é …"ï¼Œå¿…é ˆåŒ…å« "ä¸ä¿"
3. å•æ³•è¦é‡å°é€™å€‹ç‰¹å®šé …ç›®

è«‹ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
  "intents": [
    {{
      "user_query": "é‡å°æ­¤é …ç›®çš„å…·é«”å•é¡Œ",
      "excerpt": "é—œéµå…§å®¹",
      "conditions": ["æ¢ä»¶"],
      "exceptions": ["ä¾‹å¤–"],
      "referenced_clauses": ["å¼•ç”¨"],
      "category": "åˆ†é¡",
      "query_type": "æŸ¥è©¢é¡å‹",
      "semantic_tags": ["å¿…é ˆåŒ…å«å‹•ä½œé—œéµè©"],
      "difficulty": "é›£åº¦"
    }}
  ]
}}

åªè¿”å› JSONï¼š"""


# ==================== è³ªé‡é©—è­‰å™¨ ====================

class IntentQualityValidator:
    """Intent è³ªé‡é©—è­‰å™¨"""
    
    @staticmethod
    def validate_and_fix(intent: Dict, clause: Dict) -> tuple[Dict, float]:
        """
        é©—è­‰ä¸¦ä¿®æ­£ intent
        
        Returns:
            (ä¿®æ­£å¾Œçš„ intent, è³ªé‡è©•åˆ†)
        """
        quality_score = 1.0
        issues = []
        
        # 1. æª¢æŸ¥ semantic_tags æ˜¯å¦åŒ…å« action_keywords
        required_tags = clause.get("action_keywords", [])
        current_tags = intent.get("semantic_tags", [])
        
        missing_tags = [tag for tag in required_tags if tag not in current_tags]
        if missing_tags:
            intent["semantic_tags"] = current_tags + missing_tags
            quality_score -= 0.2
            issues.append(f"ç¼ºå°‘å‹•ä½œæ¨™ç±¤: {missing_tags}")
        
        # 2. æª¢æŸ¥ä¸ä¿äº‹é …
        if clause.get("clause_function") == "ä¸ä¿äº‹é …":
            if "ä¸ä¿" not in current_tags:
                intent["semantic_tags"].append("ä¸ä¿")
                quality_score -= 0.15
                issues.append("ä¸ä¿äº‹é …ç¼ºå°‘'ä¸ä¿'æ¨™ç±¤")
        
        # 3. æª¢æŸ¥ user_query æ˜¯å¦å¤ªå¯¬æ³›
        vague_patterns = ["æ€éº¼è¾¦", "å¦‚ä½•", "å¯ä»¥å—"]
        query = intent.get("user_query", "")
        if len(query) < 8 or all(p not in query for p in vague_patterns):
            # æŸ¥è©¢å¯èƒ½å¤ªç°¡å–®æˆ–å¤ªå¯¬æ³›
            if len(query) < 6:
                quality_score -= 0.1
                issues.append("æŸ¥è©¢å¤ªçŸ­")
        
        # 4. æª¢æŸ¥å°æ¯”æŸ¥è©¢
        if intent.get("query_type") == "å°æ¯”æŸ¥è©¢":
            if len(current_tags) < 2:
                quality_score -= 0.2
                issues.append("å°æ¯”æŸ¥è©¢æ‡‰åŒ…å«è‡³å°‘2å€‹èªç¾©æ¨™ç±¤")
        
        # 5. æª¢æŸ¥æ˜¯å¦æ··æ·†æ¦‚å¿µ
        confusing_pairs = [
            (["å»¶èª¤"], ["éºå¤±", "æå¤±"]),
            (["éºå¤±"], ["å»¶èª¤"]),
            (["å–æ¶ˆ"], ["æ›´æ”¹"]),
        ]
        
        for group1, group2 in confusing_pairs:
            if any(t in current_tags for t in group1) and any(t in current_tags for t in group2):
                # å¯èƒ½æ··æ·†ï¼ˆé™¤éæ˜¯å°æ¯”æŸ¥è©¢ï¼‰
                if intent.get("query_type") != "å°æ¯”æŸ¥è©¢":
                    quality_score -= 0.3
                    issues.append(f"å¯èƒ½æ··æ·†æ¦‚å¿µ: {group1} vs {group2}")
        
        # è¨˜éŒ„å•é¡Œ
        if issues:
            intent["validation_issues"] = issues
        
        return intent, quality_score


# ==================== LLM èª¿ç”¨ ====================

def call_llm_for_intents(prompt: str, max_retries: int = 3) -> List[Dict]:
    """èª¿ç”¨ LLM ç”Ÿæˆæ„åœ–"""
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯å°ˆæ¥­çš„ä¿éšªæ¢æ¬¾åˆ†æå°ˆå®¶ã€‚ä½ å¿…é ˆåš´æ ¼éµå®ˆè¦å‰‡ç”Ÿæˆé«˜è³ªé‡çš„æ„åœ–ï¼Œç‰¹åˆ¥æ³¨æ„èªç¾©æ¨™ç±¤çš„æº–ç¢ºæ€§ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,  # é™ä½æº«åº¦æé«˜ä¸€è‡´æ€§
                max_tokens=3000
            )
            
            content = response.choices[0].message.content.strip()
            
            # ç§»é™¤ markdown
            content = re.sub(r'^```json\s*\n?', '', content)
            content = re.sub(r'^```\s*\n?', '', content)
            content = re.sub(r'\n?```$', '', content)
            content = content.strip()
            
            result = json.loads(content)
            return result.get("intents", [])
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON è§£æéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                print(f"âŒ ç„¡æ³•è§£æ: {content[:200]}")
                return []
            time.sleep(1)
            
        except Exception as e:
            print(f"âš ï¸  LLM éŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                return []
            time.sleep(2)
    
    return []


# ==================== æ„åœ–ç”Ÿæˆ ====================

def generate_clause_intents(chunk: Dict, 
                           intent_id_counter: List[int],
                           validator: IntentQualityValidator) -> List[Intent]:
    """ç‚ºæ¢æ–‡ç”Ÿæˆæ„åœ–ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
    clause = chunk["clause"]
    
    # æ§‹å»º æç¤ºè©ï¼ˆåŒ…å« Few-shotï¼‰
    prompt = INTENT_GENERATION_PROMPT.format(
        clause_no=clause["clause_no"],
        clause_title=clause["clause_title"],
        clause_function=clause.get("clause_function", "ä¸€èˆ¬è¦å®š"),
        insurance_types=", ".join(clause.get("insurance_types", ["å…¶ä»–"])),
        action_keywords=", ".join(clause.get("action_keywords", [])),
        context=clause["context"],
        few_shot_examples=FEW_SHOT_EXAMPLES
    )
    
    llm_intents = call_llm_for_intents(prompt)
    
    # é©—è­‰ä¸¦ä¿®æ­£
    intents = []
    for llm_intent in llm_intents:
        # é©—è­‰
        fixed_intent, quality_score = validator.validate_and_fix(llm_intent, clause)
        
        intent_id = f"intent_{intent_id_counter[0]:04d}"
        intent_id_counter[0] += 1
        
        intents.append(Intent(
            intent_id=intent_id,
            clause_id=clause["clause_id"],
            item_no=None,
            subitem_no=None,
            user_query=fixed_intent.get("user_query", ""),
            excerpt=fixed_intent.get("excerpt", ""),
            conditions=fixed_intent.get("conditions", []),
            exceptions=fixed_intent.get("exceptions", []),
            referenced_clauses=fixed_intent.get("referenced_clauses", []),
            category=fixed_intent.get("category", "å…¶ä»–"),
            query_type=fixed_intent.get("query_type", "ç›´æ¥æŸ¥è©¢"),
            semantic_tags=fixed_intent.get("semantic_tags", []),
            difficulty=fixed_intent.get("difficulty", "ç°¡å–®"),
            quality_score=quality_score
        ))
    
    return intents


def generate_item_intents(chunk: Dict, 
                         item: Dict, 
                         intent_id_counter: List[int],
                         validator: IntentQualityValidator) -> List[Intent]:
    """ç‚ºé …ç›®ç”Ÿæˆæ„åœ–ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
    clause = chunk["clause"]
    
    prompt = ITEM_INTENT_GENERATION_PROMPT.format(
        clause_no=clause["clause_no"],
        clause_title=clause["clause_title"],
        clause_function=clause.get("clause_function", "ä¸€èˆ¬è¦å®š"),
        item_no=item["item_no"],
        action_keywords=", ".join(item.get("action_keywords", [])),
        item_context=item["context"]
    )
    
    llm_intents = call_llm_for_intents(prompt)
    
    intents = []
    for llm_intent in llm_intents:
        fixed_intent, quality_score = validator.validate_and_fix(llm_intent, clause)
        
        intent_id = f"intent_{intent_id_counter[0]:04d}"
        intent_id_counter[0] += 1
        
        intents.append(Intent(
            intent_id=intent_id,
            clause_id=clause["clause_id"],
            item_no=item["item_no"],
            subitem_no=None,
            user_query=fixed_intent.get("user_query", ""),
            excerpt=fixed_intent.get("excerpt", ""),
            conditions=fixed_intent.get("conditions", []),
            exceptions=fixed_intent.get("exceptions", []),
            referenced_clauses=fixed_intent.get("referenced_clauses", []),
            category=fixed_intent.get("category", "å…¶ä»–"),
            query_type=fixed_intent.get("query_type", "ç›´æ¥æŸ¥è©¢"),
            semantic_tags=fixed_intent.get("semantic_tags", []),
            difficulty=fixed_intent.get("difficulty", "ç°¡å–®"),
            quality_score=quality_score
        ))
    
    return intents


# ==================== å°æ¯”æ„åœ–ç”Ÿæˆ ====================

COMPARISON_INTENT_TEMPLATE = {
    "è¡Œæå»¶èª¤_vs_è¡Œææå¤±": {
        "user_query": "è¡Œæå»¶èª¤å’Œè¡Œæéºå¤±æœ‰ä»€éº¼å·®åˆ¥ï¼Ÿä»€éº¼æ™‚å€™ç®—å»¶èª¤ï¼Œä»€éº¼æ™‚å€™ç®—éºå¤±ï¼Ÿ",
        "excerpt": "è¡Œæå»¶èª¤æ˜¯æŒ‡æŠµé”6å°æ™‚å¾Œä»æœªé ˜å¾—ï¼›è¡Œææå¤±æ˜¯æŒ‡æ¯€æã€æ»…å¤±æˆ–éºå¤±",
        "category": "å®šç¾©èªªæ˜",
        "query_type": "å°æ¯”æŸ¥è©¢",
        "semantic_tags": ["å»¶èª¤", "éºå¤±", "å°æ¯”"],
        "difficulty": "ä¸­ç­‰",
        "related_clauses": ["ç¬¬ä¸‰åå…­æ¢", "ç¬¬ä¸‰åä¹æ¢"]
    },
    "ç­æ©Ÿå»¶èª¤_vs_æ—…ç¨‹å–æ¶ˆ": {
        "user_query": "ç­æ©Ÿå»¶èª¤å’Œæ—…ç¨‹å–æ¶ˆæœ‰ä»€éº¼ä¸åŒï¼Ÿåˆ†åˆ¥åœ¨ä»€éº¼æƒ…æ³ä¸‹ç†è³ ï¼Ÿ",
        "excerpt": "ç­æ©Ÿå»¶èª¤æ˜¯ç­æ©Ÿæ™šé»ï¼›æ—…ç¨‹å–æ¶ˆæ˜¯åœ¨å‡ºç™¼å‰å› ç‰¹å®šäº‹ç”±å–æ¶ˆæ•´å€‹è¡Œç¨‹",
        "category": "å®šç¾©èªªæ˜",
        "query_type": "å°æ¯”æŸ¥è©¢",
        "semantic_tags": ["å»¶èª¤", "å–æ¶ˆ", "å°æ¯”"],
        "difficulty": "ä¸­ç­‰",
        "related_clauses": ["ç¬¬äºŒåä¸ƒæ¢", "ç¬¬ä¸‰åæ¢"]
    },
    "æ—…ç¨‹å–æ¶ˆ_vs_æ—…ç¨‹æ›´æ”¹": {
        "user_query": "æ—…ç¨‹å–æ¶ˆå’Œæ—…ç¨‹æ›´æ”¹æœ‰ä½•å·®åˆ¥ï¼Ÿ",
        "excerpt": "æ—…ç¨‹å–æ¶ˆæ˜¯å‡ºç™¼å‰å…¨éƒ¨å–æ¶ˆï¼›æ—…ç¨‹æ›´æ”¹æ˜¯æ—…è¡Œä¸­å› æ•…è®Šæ›´è¡Œç¨‹",
        "category": "å®šç¾©èªªæ˜",
        "query_type": "å°æ¯”æŸ¥è©¢",
        "semantic_tags": ["å–æ¶ˆ", "æ›´æ”¹", "å°æ¯”"],
        "difficulty": "ä¸­ç­‰",
        "related_clauses": ["ç¬¬äºŒåä¸ƒæ¢", "ç¬¬ä¸‰åä¸‰æ¢"]
    },
    "ç«Šç›œ_vs_è™•ç†å¤±ç•¶": {
        "user_query": "è¡Œæè¢«å·å’Œè¢«èˆªç©ºå…¬å¸å¼„ä¸Ÿï¼Œç†è³ æœ‰ä»€éº¼ä¸åŒï¼Ÿ",
        "excerpt": "ç«Šç›œéœ€å ±è­¦ä¸¦å–å¾—å ±æ¡ˆè­‰æ˜ï¼›è™•ç†å¤±ç•¶éœ€æ¥­è€…å‡ºå…·äº‹æ•…è­‰æ˜",
        "category": "ç”³è«‹æµç¨‹",
        "query_type": "å°æ¯”æŸ¥è©¢",
        "semantic_tags": ["ç«Šç›œ", "éºå¤±", "å°æ¯”"],
        "difficulty": "ä¸­ç­‰",
        "related_clauses": ["ç¬¬ä¸‰åä¹æ¢", "ç¬¬å››åäºŒæ¢", "ç¬¬å››åä¸‰æ¢"]
    }
}


def generate_comparison_intents(chunks: List[Dict], intent_id_counter: List[int]) -> List[Intent]:
    """ç”Ÿæˆå°æ¯”æ„åœ–"""
    intents = []
    
    for comp_key, comp_data in COMPARISON_INTENT_TEMPLATE.items():
        intent_id = f"intent_{intent_id_counter[0]:04d}"
        intent_id_counter[0] += 1
        
        related_clauses = comp_data.get("related_clauses", [])
        main_clause_id = chunks[0]["clause"]["clause_id"] if chunks else "ç¬¬ä¸€æ¢"
        
        for chunk in chunks:
            clause_no = chunk["clause"]["clause_no"]
            if related_clauses and clause_no in related_clauses[0]:
                main_clause_id = chunk["clause"]["clause_id"]
                break
        
        intents.append(Intent(
            intent_id=intent_id,
            clause_id=main_clause_id,
            item_no=None,
            subitem_no=None,
            user_query=comp_data["user_query"],
            excerpt=comp_data["excerpt"],
            conditions=[],
            exceptions=[],
            referenced_clauses=related_clauses,
            category=comp_data["category"],
            query_type=comp_data["query_type"],
            semantic_tags=comp_data["semantic_tags"],
            difficulty=comp_data["difficulty"],
            quality_score=1.0
        ))
    
    print(f"âœ… å·²ç”Ÿæˆ {len(intents)} å€‹å°æ¯”æ„åœ–")
    return intents


# ==================== ä¸»å‡½æ•¸ ====================

def generate_all_intents(chunks: List[Dict], 
                         generate_for_items: bool = True,
                         generate_comparisons: bool = True) -> List[Intent]:
    """ç”Ÿæˆæ‰€æœ‰æ„åœ–ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
    all_intents = []
    intent_id_counter = [1]
    validator = IntentQualityValidator()
    
    total_chunks = len(chunks)
    low_quality_count = 0
    
    # 1. ç”Ÿæˆæ¢æ–‡å’Œé …ç›®æ„åœ–
    for i, chunk in enumerate(chunks, 1):
        clause = chunk["clause"]
        print(f"ğŸ”„ è™•ç† [{i}/{total_chunks}]: {clause['clause_no']} {clause['clause_title']}")
        
        # æ¢æ–‡ç´šåˆ¥æ„åœ–
        clause_intents = generate_clause_intents(chunk, intent_id_counter, validator)
        all_intents.extend(clause_intents)
        
        # çµ±è¨ˆä½è³ªé‡ intent
        low_quality = [i for i in clause_intents if i.quality_score < 0.8]
        if low_quality:
            low_quality_count += len(low_quality)
            print(f"  âš ï¸  {len(low_quality)} å€‹ä½è³ªé‡ intentï¼ˆå·²è‡ªå‹•ä¿®æ­£ï¼‰")
        
        # é …ç›®ç´šåˆ¥æ„åœ–
        if generate_for_items and clause.get("items"):
            for item in clause["items"]:
                item_intents = generate_item_intents(chunk, item, intent_id_counter, validator)
                all_intents.extend(item_intents)
                item["intent_ids"] = [intent.intent_id for intent in item_intents]
        
        clause["intent_ids"] = [intent.intent_id for intent in clause_intents]
        
        time.sleep(0.5)
    
    # 2. ç”Ÿæˆå°æ¯”æ„åœ–
    if generate_comparisons:
        print("\nğŸ”„ ç”Ÿæˆå°æ¯”æ„åœ–...")
        comparison_intents = generate_comparison_intents(chunks, intent_id_counter)
        all_intents.extend(comparison_intents)
    
    print(f"\nâœ… ç¸½å…±ç”Ÿæˆ {len(all_intents)} å€‹æ„åœ–")
    print(f"   - ä½è³ªé‡ï¼ˆå·²ä¿®æ­£ï¼‰: {low_quality_count}")
    print(f"   - å¹³å‡è³ªé‡åˆ†æ•¸: {sum(i.quality_score for i in all_intents) / len(all_intents):.2f}")
    
    # çµ±è¨ˆ
    query_types = {}
    quality_distribution = {"å„ªç§€(>0.9)": 0, "è‰¯å¥½(0.8-0.9)": 0, "ä¸­ç­‰(<0.8)": 0}
    
    for intent in all_intents:
        qt = intent.query_type
        query_types[qt] = query_types.get(qt, 0) + 1
        
        if intent.quality_score > 0.9:
            quality_distribution["å„ªç§€(>0.9)"] += 1
        elif intent.quality_score >= 0.8:
            quality_distribution["è‰¯å¥½(0.8-0.9)"] += 1
        else:
            quality_distribution["ä¸­ç­‰(<0.8)"] += 1
    
    print(f"\nğŸ“Š æ„åœ–é¡å‹åˆ†å¸ƒ:")
    for qt, count in sorted(query_types.items(), key=lambda x: x[1], reverse=True):
        print(f"   - {qt}: {count}")
    
    print(f"\nğŸ“Š è³ªé‡åˆ†å¸ƒ:")
    for level, count in quality_distribution.items():
        print(f"   - {level}: {count}")
    
    return all_intents


# ==================== ä¿å­˜å‡½æ•¸ ====================

def save_intents(intents: List[Intent], output_path: str):
    """ä¿å­˜æ„åœ–åˆ° JSON æ–‡ä»¶"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    data = {
        "metadata": {
            "total_intents": len(intents),
            "generated_at": __import__('datetime').datetime.now().isoformat(),
            "version": "3.0",
            "enhancements": [
                "Few-shot Learningï¼ˆæ­£åä¾‹ï¼‰",
                "å¾Œè™•ç†é©—è­‰èˆ‡è‡ªå‹•ä¿®æ­£",
                "è³ªé‡è©•åˆ†æ©Ÿåˆ¶",
                "æ›´åš´æ ¼çš„èªç¾©æ¨™ç±¤è¦æ±‚"
            ],
            "average_quality_score": sum(i.quality_score for i in intents) / len(intents)
        },
        "intents": [intent.to_dict() for intent in intents]
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜ {len(intents)} å€‹æ„åœ–è‡³ {output_path}")


def save_chunks_with_intents(chunks: List[Dict], output_path: str):
    """ä¿å­˜åŒ…å«æ„åœ– ID çš„ chunks"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    data = {
        "metadata": {
            "total_chunks": len(chunks),
            "generated_at": __import__('datetime').datetime.now().isoformat(),
            "version": "3.0"
        },
        "chunks": chunks
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜ chunksï¼ˆå«æ„åœ–ï¼‰è‡³ {output_path}")


# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    print("ğŸ”„ é–‹å§‹ç”Ÿæˆæ„åœ–ï¼ˆå„ªåŒ–ç‰ˆï¼‰...")
    print("æ”¹é€²: Few-shot Learning + å¾Œè™•ç†é©—è­‰ + è³ªé‡è©•åˆ†\n")
    
    # è¼‰å…¥ chunks
    chunks_path = os.path.join(INDEX_DIR, "chunks_structured.json")
    
    if not os.path.exists(chunks_path):
        print(f"âŒ æ‰¾ä¸åˆ° chunks æ–‡ä»¶: {chunks_path}")
        print("è«‹å…ˆé‹è¡Œ chunk_generator.py")
        exit(1)
    
    with open(chunks_path, "r", encoding="utf-8") as f:
        chunks_data = json.load(f)
    
    chunks = chunks_data["chunks"]
    print(f"ğŸ“¥ å·²è¼‰å…¥ {len(chunks)} å€‹ chunks")
    
    # ç”Ÿæˆæ„åœ–
    intents = generate_all_intents(
        chunks, 
        generate_for_items=True,
        generate_comparisons=True
    )
    
    # ä¿å­˜
    intents_path = os.path.join(INDEX_DIR, "intents.json")
    save_intents(intents, intents_path)
    
    chunks_with_intents_path = os.path.join(INDEX_DIR, "chunks_structured_with_intents.json")
    save_chunks_with_intents(chunks, chunks_with_intents_path)
    
    # è©³ç´°çµ±è¨ˆ
    categories = {}
    semantic_tags_count = {}
    
    for intent in intents:
        cat = intent.category
        categories[cat] = categories.get(cat, 0) + 1
        
        for tag in intent.semantic_tags:
            semantic_tags_count[tag] = semantic_tags_count.get(tag, 0) + 1
    
    print("\nğŸ“Š æ„åœ–åˆ†é¡çµ±è¨ˆ:")
    for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True)[:15]:
        print(f"   - {cat}: {count}")
    
    print("\nğŸ·ï¸  èªç¾©æ¨™ç±¤çµ±è¨ˆ:")
    for tag, count in sorted(semantic_tags_count.items(), key=lambda x: x[1], reverse=True)[:15]:
        print(f"   - {tag}: {count}")

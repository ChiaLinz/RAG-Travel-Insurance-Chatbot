"""
Answer Generator - åŸºæ–¼æª¢ç´¢çµæœç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ

âœ… ä¿®å¾©ï¼šç›¸ä¼¼åº¦é¡¯ç¤ºå•é¡Œï¼ˆå„ªå…ˆä½¿ç”¨ rerank_scoreï¼‰

åŠŸèƒ½ï¼š
1. å¾æª¢ç´¢å¼•æ“ç²å–ç›¸é—œæ¢æ–‡
2. æ§‹å»ºçµæ§‹åŒ–æç¤ºè©
3. èª¿ç”¨ LLM ç”Ÿæˆå°ˆæ¥­ç­”æ¡ˆ
4. æ”¯æŒå¤šè¼ªå°è©±ï¼ˆå¯é¸ï¼‰
"""

import json
import os
from typing import List, Dict, Optional
from openai import OpenAI
from dotenv import load_dotenv
from core.retrieval_engine import RetrievalEngine
from config import INDEX_DIR


# ==================== åˆå§‹åŒ– ====================

load_dotenv()
client = OpenAI()


# ==================== æç¤ºè©æ¨¡æ¿ ====================

SYSTEM_PROMPT = """ä½ æ˜¯å°ˆæ¥­çš„æ—…éŠä¿éšªæ¢æ¬¾å•ç­”å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæä¾›çš„ä¿éšªæ¢æ–‡ï¼Œæº–ç¢ºã€æ¸…æ™°åœ°å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

å›ç­”è¦æ±‚ï¼š
1. **æº–ç¢ºæ€§**: åš´æ ¼åŸºæ–¼æä¾›çš„æ¢æ–‡å…§å®¹ï¼Œä¸è¦ç·¨é€ ä¿¡æ¯
2. **çµæ§‹åŒ–**: ä½¿ç”¨æ¸…æ™°çš„æ®µè½å’Œæ¢åˆ—å¼èªªæ˜
3. **å¼•ç”¨ä¾†æº**: æ˜ç¢ºæ¨™è¨»å¼•ç”¨çš„æ¢æ–‡ç·¨è™Ÿå’Œé …ç›®
4. **å®Œæ•´æ€§**: åŒ…å«é©ç”¨æ¢ä»¶ã€ä¾‹å¤–æƒ…æ³ã€æ³¨æ„äº‹é …ç­‰
5. **æ˜“è®€æ€§**: ä½¿ç”¨ç°¡å–®æ˜ç­çš„èªè¨€ï¼Œé¿å…éåº¦å°ˆæ¥­è¡“èª

å›ç­”æ ¼å¼å»ºè­°ï¼š
- å…ˆçµ¦å‡ºç°¡çŸ­çš„ç›´æ¥ç­”æ¡ˆ
- ç„¶å¾Œè©³ç´°èªªæ˜æ¢ä»¶å’Œç´°ç¯€
- æœ€å¾Œè£œå……ä¾‹å¤–æƒ…æ³æˆ–æ³¨æ„äº‹é …
- æ¯å€‹è¦é»éƒ½æ¨™è¨»ä¾†æºæ¢æ–‡

èªæ°£ï¼šå°ˆæ¥­ã€å‹å–„ã€è€å¿ƒ"""


USER_PROMPT_TEMPLATE = """è«‹æ ¹æ“šä»¥ä¸‹ä¿éšªæ¢æ–‡å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

ã€ç›¸é—œæ¢æ–‡ã€‘
{context}

ã€ä½¿ç”¨è€…å•é¡Œã€‘
{query}

è«‹æä¾›è©³ç´°ä¸”çµæ§‹åŒ–çš„å›ç­”ã€‚"""


# ==================== ç­”æ¡ˆç”Ÿæˆå™¨ ====================

class AnswerGenerator:
    """ç­”æ¡ˆç”Ÿæˆå™¨"""
    
    def __init__(self, 
                 retrieval_engine: RetrievalEngine,
                 model: str = "gpt-4o-mini",
                 temperature: float = 0.1):
        """
        åˆå§‹åŒ–ç­”æ¡ˆç”Ÿæˆå™¨
        
        Args:
            retrieval_engine: æª¢ç´¢å¼•æ“å¯¦ä¾‹
            model: OpenAI æ¨¡å‹åç¨±
            temperature: ç”Ÿæˆæº«åº¦
        """
        self.retrieval_engine = retrieval_engine
        self.model = model
        self.temperature = temperature
    
    def generate(self,
                query: str,
                top_k_intents: int = 5,
                top_k_clauses: int = 3,
                include_sources: bool = True) -> Dict:
        """
        ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            top_k_intents: æª¢ç´¢å‰ K å€‹æ„åœ–
            top_k_clauses: ä½¿ç”¨å‰ K å€‹æ¢æ–‡
            include_sources: æ˜¯å¦åœ¨éŸ¿æ‡‰ä¸­åŒ…å«ä¾†æºä¿¡æ¯
        
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œå…ƒæ•¸æ“šçš„å­—å…¸
        """
        # æª¢ç´¢ç›¸é—œæ¢æ–‡
        retrieval_result = self.retrieval_engine.retrieve(
            query,
            top_k_intents=top_k_intents,
            top_k_clauses=top_k_clauses,
            include_metadata=True
        )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context = self._format_context(retrieval_result["top_clauses"])
        
        # æ§‹å»ºæç¤ºè©
        user_prompt = USER_PROMPT_TEMPLATE.format(
            context=context,
            query=query
        )
        
        # èª¿ç”¨ LLM
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.temperature,
                max_tokens=2000
            )
            
            answer = response.choices[0].message.content
            
            # æ§‹å»ºçµæœ
            result = {
                "query": query,
                "answer": answer,
                "model": self.model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
            
            if include_sources:
                result["sources"] = self._extract_sources(retrieval_result["top_clauses"])
                result["top_intents"] = [
                    {
                        "intent_id": intent["intent_id"],
                        "user_query": intent["intent_data"]["user_query"],
                        "category": intent["intent_data"]["category"],
                        # âœ… ä¿®å¾©ï¼šå„ªå…ˆä½¿ç”¨ hybrid_score
                        "similarity": intent.get("hybrid_score", intent.get("similarity_score", 0))
                    }
                    for intent in retrieval_result["top_intents"]
                ]
            
            return result
            
        except Exception as e:
            return {
                "query": query,
                "answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                "error": str(e)
            }
    
    def _format_context(self, clauses: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–æ¢æ–‡ç‚ºä¸Šä¸‹æ–‡
        
        Args:
            clauses: æ¢æ–‡åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        context_parts = []
        
        for i, clause in enumerate(clauses, 1):
            # ä¾†æºé¡å‹æ¨™ç±¤
            source_label = {
                "mother": "ã€æ¯æ¢æ–‡ã€‘",
                "item": "ã€å­é …ç›®ã€‘",
                "subitem": "ã€å­æ¬¾é …ã€‘",
                "referenced": "ã€å¼•ç”¨æ¢æ–‡ã€‘"
            }.get(clause["source_type"], "ã€å…¶ä»–ã€‘")
            
            # ä½ç½®ä¿¡æ¯
            location = clause["clause_id"]
            if clause.get("item_no"):
                location += f" ç¬¬{clause['item_no']}é …"
            if clause.get("subitem_no"):
                location += f" ({clause['subitem_no']})"
            
            # çµ„åˆ
            context_parts.append(
                f"{source_label} {location}\n"
                f"{clause['content']}\n"
            )
        
        return "\n".join(context_parts)
    
    def _extract_sources(self, clauses: List[Dict]) -> List[Dict]:
        """
        æå–ä¾†æºä¿¡æ¯
        
        âœ… ä¿®å¾©ï¼šå„ªå…ˆä½¿ç”¨ rerank_scoreï¼Œfallback åˆ° similarity_score
        
        Args:
            clauses: æ¢æ–‡åˆ—è¡¨
        
        Returns:
            ä¾†æºä¿¡æ¯åˆ—è¡¨
        """
        sources = []
        
        for clause in clauses:
            # âœ… é—œéµä¿®å¾©ï¼šå„ªå…ˆè®€å– rerank_score
            score = clause.get("rerank_score", clause.get("similarity_score", 0.0))
            
            source = {
                "clause_id": clause["clause_id"],
                "source_type": clause["source_type"],
                "similarity_score": score  # ä¿æŒå­—æ®µåç¨±ä¸€è‡´
            }
            
            if clause.get("item_no"):
                source["item_no"] = clause["item_no"]
            if clause.get("subitem_no"):
                source["subitem_no"] = clause["subitem_no"]
            
            sources.append(source)
        
        return sources


# ==================== å°è©±å¼ç”Ÿæˆå™¨ï¼ˆå¯é¸ï¼‰ ====================

class ConversationalAnswerGenerator(AnswerGenerator):
    """æ”¯æŒå¤šè¼ªå°è©±çš„ç­”æ¡ˆç”Ÿæˆå™¨"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.conversation_history = []
    
    def generate_with_history(self,
                              query: str,
                              **kwargs) -> Dict:
        """
        åŸºæ–¼æ­·å²å°è©±ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            **kwargs: å…¶ä»–åƒæ•¸
        
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œå…ƒæ•¸æ“šçš„å­—å…¸
        """
        # æª¢ç´¢ç›¸é—œæ¢æ–‡
        retrieval_result = self.retrieval_engine.retrieve(
            query,
            top_k_intents=kwargs.get('top_k_intents', 5),
            top_k_clauses=kwargs.get('top_k_clauses', 3),
            include_metadata=True
        )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context = self._format_context(retrieval_result["top_clauses"])
        
        # æ§‹å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        
        # æ·»åŠ æ­·å²å°è©±
        messages.extend(self.conversation_history)
        
        # æ·»åŠ ç•¶å‰æŸ¥è©¢
        current_prompt = USER_PROMPT_TEMPLATE.format(
            context=context,
            query=query
        )
        messages.append({"role": "user", "content": current_prompt})
        
        # èª¿ç”¨ LLM
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=2000
            )
            
            answer = response.choices[0].message.content
            
            # æ›´æ–°æ­·å²
            self.conversation_history.append({"role": "user", "content": query})
            self.conversation_history.append({"role": "assistant", "content": answer})
            
            # ä¿æŒæ­·å²é•·åº¦ï¼ˆæœ€å¤šä¿ç•™æœ€è¿‘ 10 è¼ªï¼‰
            if len(self.conversation_history) > 20:
                self.conversation_history = self.conversation_history[-20:]
            
            return {
                "query": query,
                "answer": answer,
                "model": self.model,
                "conversation_length": len(self.conversation_history) // 2,
                "sources": self._extract_sources(retrieval_result["top_clauses"]),
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
            
        except Exception as e:
            return {
                "query": query,
                "answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                "error": str(e)
            }
    
    def reset_history(self):
        """é‡ç½®å°è©±æ­·å²"""
        self.conversation_history = []


# ==================== ä¸»ç¨‹åºï¼ˆæ¸¬è©¦ï¼‰ ====================

if __name__ == "__main__":
    # åˆå§‹åŒ–æª¢ç´¢å¼•æ“
    intents_path = os.path.join(INDEX_DIR, "intents.json")
    chunks_path = os.path.join(INDEX_DIR, "chunks_structured_with_intents.json")
    
    if not os.path.exists(intents_path) or not os.path.exists(chunks_path):
        print("âŒ è«‹å…ˆé‹è¡Œ chunk_generator.py å’Œ intent_generator.py")
        exit(1)
    
    retrieval_engine = RetrievalEngine(
        intents_path, 
        chunks_path,
        use_bm25=True,
        use_cross_encoder=True
    )
    
    # åˆå§‹åŒ–ç­”æ¡ˆç”Ÿæˆå™¨
    answer_gen = AnswerGenerator(retrieval_engine)
    
    # æ¸¬è©¦æŸ¥è©¢
    test_queries = [
        "ä»€éº¼æƒ…æ³ä¸‹å¯ä»¥ç”³è«‹æ—…éŠå»¶èª¤è³ å„Ÿï¼Ÿ",
        "è¡Œæéºå¤±å¾Œæ‡‰è©²å¦‚ä½•ç”³è«‹ç†è³ ï¼Ÿ",
        "å“ªäº›åŸå› å±¬æ–¼ä¸å¯ç†è³ ç¯„åœï¼Ÿ",
        "ç­æ©Ÿå»¶èª¤å¤šä¹…å¯ä»¥ç†è³ ï¼Ÿ",
    ]
    
    print("\n" + "="*80)
    print("ğŸ¤– æ—…éŠä¿éšªå•ç­”ç³»çµ±æ¸¬è©¦")
    print("="*80)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nã€å•é¡Œ {i}ã€‘{query}")
        print("-" * 80)
        
        result = answer_gen.generate(
            query,
            top_k_intents=5,
            top_k_clauses=3,
            include_sources=True
        )
        
        print(f"\n{result['answer']}")
        
        if 'sources' in result:
            print("\nğŸ“š åƒè€ƒæ¢æ–‡:")
            for source in result['sources']:
                location = source['clause_id']
                if source.get('item_no'):
                    location += f" ç¬¬{source['item_no']}é …"
                print(f"  - {location} (ç›¸ä¼¼åº¦: {source['similarity_score']:.3f})")
        
        if 'usage' in result:
            print(f"\nğŸ’¡ Token ä½¿ç”¨: {result['usage']['total_tokens']} "
                  f"(prompt: {result['usage']['prompt_tokens']}, "
                  f"completion: {result['usage']['completion_tokens']})")
        
        print("=" * 80)